{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full Pipeline: PubMedBERT Data Extraction Example\n",
    "\n",
    "This script demonstrates:\n",
    "  1. Loading PubMedBERT (a masked language model).\n",
    "  2. Loading a local subset of PubMed data (papers.json).\n",
    "  3. Generating candidate text with repeated fill-mask tokens.\n",
    "  4. Applying a naive membership-inference-style filter\n",
    "     using zlib compression ratio as a proxy for \"unusually confident\" text.\n",
    "  5. Searching the local PubMed data to see if the text is indeed memorized\n",
    "     (verbatim match).\n",
    "\n",
    "DISCLAIMER:\n",
    "  - PubMedBERT is not an auto-regressive model. Generating free-form\n",
    "    text is tricky. We do a repeated fill-mask approach for demonstration.\n",
    "  - The membership inference here is simplified. Real approaches might\n",
    "    compare perplexities from multiple models or do more advanced metrics.\n",
    "  - The substring search is naive and may need optimization or fuzzy matching.\n",
    "  - This code is a proof-of-concept. Modify and expand to suit your needs.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import zlib\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# STEP 1: Load Model & Data\n",
    "###############################################################################\n",
    "\n",
    "MODEL_NAME = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    "PUBMED_JSON_PATH = \"papersOld.json\"\n",
    "\n",
    "def load_pubmed_data(json_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Loads local PubMed data from a JSON file and returns a list of paper dicts.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"[ERROR] {json_path} not found.\")\n",
    "        return []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading local papers.json data ===\n",
      "[INFO] Loaded 2112 records from papersOld.json.\n",
      "[INFO] Found 1934 records with non-empty abstracts.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "print(\"=== Loading local papers.json data ===\")\n",
    "papers_data = load_pubmed_data(PUBMED_JSON_PATH)\n",
    "print(f\"[INFO] Loaded {len(papers_data)} records from {PUBMED_JSON_PATH}.\")\n",
    "# Optional: Some simple stats\n",
    "abstract_count = sum(1 for p in papers_data if p.get(\"abstract\", {}).get(\"full_text\"))\n",
    "print(f\"[INFO] Found {abstract_count} records with non-empty abstracts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Collated 1934 candidate abstracts for prompts.\n"
     ]
    }
   ],
   "source": [
    "abstract_texts = []\n",
    "for paper in papers_data:\n",
    "    txt = paper.get(\"abstract\", {}).get(\"full_text\", \"\").strip()\n",
    "    if txt:\n",
    "        abstract_texts.append(txt)\n",
    "print(f\"[INFO] Collated {len(abstract_texts)} candidate abstracts for prompts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 2: Generating Candidate Text (Fill-Mask) ===\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# STEP 2: Generate Candidate Text\n",
    "############################################################################\n",
    "print(\"\\n=== Step 2: Generating Candidate Text (Fill-Mask) ===\")\n",
    "# We'll create a fill-mask pipeline. With top_k=50 for diversity.\n",
    "fill_mask_pipe = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    top_k=50,\n",
    "    device=0 if torch.cuda.is_available() else -1  # use GPU if available\n",
    ")\n",
    "# Let’s define how many sequences to generate\n",
    "NUM_GENERATIONS = 1000  # Increase for a real experiment\n",
    "MASK_LENGTH = 5        # Number of mask tokens to fill per generation\n",
    "MAX_PROMPT_WORDS = 8   # We'll slice up to 8 words from a random abstract\n",
    "generated_samples = []\n",
    "def repeated_fill_mask(prompt_text: str, num_masks: int, fill_mask_pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Iteratively fill 'num_masks' occurrences of [MASK] in 'prompt_text' using a\n",
    "    fill-mask pipeline from Hugging Face. One mask is filled at a time, to avoid\n",
    "    nested-list outputs.\n",
    "\n",
    "    Args:\n",
    "        prompt_text (str): The initial string (which may contain multiple [MASK] tokens).\n",
    "        num_masks (int): How many [MASK] tokens we want to fill in total.\n",
    "        fill_mask_pipeline: A fill-mask pipeline (e.g., from transformers.pipeline(\"fill-mask\")).\n",
    "\n",
    "    Returns:\n",
    "        str: The final string after attempting to fill up to num_masks [MASK] tokens.\n",
    "    \"\"\"\n",
    "    sequence = prompt_text\n",
    "\n",
    "    for _ in range(num_masks):\n",
    "        # 1) Find the first [MASK] in the text\n",
    "        first_mask_index = sequence.find(\"[MASK]\")\n",
    "        if first_mask_index == -1:\n",
    "            # No more [MASK] found\n",
    "            break\n",
    "\n",
    "        # 2) We feed the entire string (with multiple [MASK] tokens) to the pipeline.\n",
    "        #    By design, if it sees multiple masks, it returns a list of lists:\n",
    "        #    e.g. results[0] are the top-k fills for the first mask, results[1] for the second, etc.\n",
    "        results = fill_mask_pipeline(sequence)\n",
    "\n",
    "        # If it's empty or None, just break\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        # Check if we got a nested list-of-lists (the multiple-mask scenario)\n",
    "        if isinstance(results[0], list):\n",
    "            # This means the pipeline gave us something like [ [dict, dict, ...], [dict, ...], ... ]\n",
    "            # focusing on results[0] => top-k fills for the first mask\n",
    "            first_mask_candidates = results[0]\n",
    "        else:\n",
    "            # Only one mask in the input => pipeline returned a single list of dict\n",
    "            first_mask_candidates = results\n",
    "\n",
    "        if not first_mask_candidates:\n",
    "            break\n",
    "\n",
    "        # 3) Choose a random fill from top_k for the FIRST mask\n",
    "        chosen_fill = random.choice(first_mask_candidates)\n",
    "        chosen_token_str = chosen_fill[\"token_str\"]  # The actual text that replaces [MASK]\n",
    "\n",
    "        # 4) Replace ONLY the first [MASK] with chosen_token_str in the string\n",
    "        #    We do a direct textual replacement for that single occurrence\n",
    "        sequence = (\n",
    "            sequence[:first_mask_index]\n",
    "            + chosen_token_str\n",
    "            + sequence[first_mask_index + len(\"[MASK]\") :]\n",
    "        )\n",
    "\n",
    "    return sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generated 1000 samples. Written to generated_pubmedbert_samples.json.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(NUM_GENERATIONS):\n",
    "    # 1) Pick a random short slice from a random abstract\n",
    "    if abstract_texts:\n",
    "        chosen_abs = random.choice(abstract_texts)\n",
    "        words = chosen_abs.split()\n",
    "        if len(words) >= 2:\n",
    "            start_idx = random.randint(0, max(len(words) - 1, 1))\n",
    "            end_idx = min(len(words), start_idx + random.randint(2, MAX_PROMPT_WORDS))\n",
    "            prompt_slice = \" \".join(words[start_idx:end_idx])\n",
    "        else:\n",
    "            prompt_slice = chosen_abs\n",
    "    else:\n",
    "        prompt_slice = \"[CLS]\"  # fallback if no data\n",
    "    # 2) Add [MASK] tokens\n",
    "    masked_prompt = prompt_slice + \" \" + \" \".join([\"[MASK]\"] * MASK_LENGTH)\n",
    "    # 3) Fill them one at a time\n",
    "    generated_text = repeated_fill_mask(masked_prompt, MASK_LENGTH, fill_mask_pipe)\n",
    "    # Store the results\n",
    "    generated_samples.append({\n",
    "        \"original_prompt\": prompt_slice,\n",
    "        \"masked_prompt\": masked_prompt,\n",
    "        \"completed_sequence\": generated_text\n",
    "    })\n",
    "# Save the raw generations\n",
    "generated_file = \"generated_pubmedbert_samples.json\"\n",
    "with open(generated_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(generated_samples, f, indent=2)\n",
    "print(f\"[INFO] Generated {len(generated_samples)} samples. Written to {generated_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 3: Membership Inference (Naive zlib approach) ===\n",
      "[INFO] 10 Most 'Suspicious' Samples by zlib ratio:\n",
      "1. ratio=0.7037 | have a with i < 7 ?\n",
      "2. ratio=0.7143 | with a from ; & an &\n",
      "3. ratio=0.7143 | ex ante. 6 : 62 fr -\n",
      "4. ratio=0.7143 | two types ? 5 ) : no\n",
      "5. ratio=0.7222 | 1-8 years. • test no > for\n",
      "6. ratio=0.7241 | readiness. / ~ r * gr\n",
      "7. ratio=0.7241 | cancel mri < i $ 16 0\n",
      "8. ratio=0.7333 | component of = g 1 0 g\n",
      "9. ratio=0.7333 | to confront . : 2 in '\n",
      "10. ratio=0.7419 | concerning f2 , 3 ? 4 t\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# STEP 3: Naive Membership Inference with zlib\n",
    "############################################################################\n",
    "print(\"\\n=== Step 3: Membership Inference (Naive zlib approach) ===\")\n",
    "# The logic: If PubMedBERT is \"unusually confident\", it might produce text\n",
    "# that compresses poorly but is presumably assigned high probability by the model.\n",
    "#\n",
    "# We'll define a simple \"zlib_score\" = len(string) / compressed_len(string),\n",
    "# meaning \"how well does the string compress?\" If the string is very random,\n",
    "# it might compress less. We suspect \"memorized\" text might be somewhat \"structured\"\n",
    "# or repeated. This is a simplistic proxy.\n",
    "def zlib_ratio(txt: str) -> float:\n",
    "    if not txt:\n",
    "        return 0.0\n",
    "    compressed = zlib.compress(txt.encode(\"utf-8\"))\n",
    "    ratio = len(txt) / len(compressed)\n",
    "    return ratio\n",
    "# We'll compute a ratio for each completed sequence, then pick the top 10 with\n",
    "# the *lowest* ratio as suspicious. \n",
    "# Actually, Carlini's approach often wants \"lowest perplexity\" => \"lowest ratio\"\n",
    "# or \"lowest compressed length.\" It's quite heuristic. \n",
    "# We'll just demonstrate one approach.\n",
    "\n",
    "extended_samples = []\n",
    "for samp in generated_samples:\n",
    "    comp_seq = samp[\"completed_sequence\"]\n",
    "    ratio = zlib_ratio(comp_seq)\n",
    "    samp[\"zlib_ratio\"] = ratio\n",
    "    extended_samples.append(samp)\n",
    "# Sort by ratio ascending (lowest ratio => more suspicious in this naive approach)\n",
    "extended_samples.sort(key=lambda x: x[\"zlib_ratio\"])\n",
    "suspicious_samples = extended_samples[:10]  # top-10 suspicious\n",
    "print(\"[INFO] 10 Most 'Suspicious' Samples by zlib ratio:\")\n",
    "for i, s in enumerate(suspicious_samples, start=1):\n",
    "    print(f\"{i}. ratio={s['zlib_ratio']:.4f} | {s['completed_sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 4: Searching for Verbatim Matches in Local PubMed Data ===\n",
      "[INFO] Verified Memorized Samples (Found in local corpus):\n",
      " None found. Possibly no direct matches in your subset, or you need more data.\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# STEP 4: Verify Memorization via Substring Search in Local Data\n",
    "############################################################################\n",
    "print(\"\\n=== Step 4: Searching for Verbatim Matches in Local PubMed Data ===\")\n",
    "def is_substring_in_paper(text: str, paper: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check if 'text' is in the paper's title or abstract or references, ignoring case.\n",
    "    \"\"\"\n",
    "    title = paper.get(\"title\", {}).get(\"full_text\", \"\")\n",
    "    abstract = paper.get(\"abstract\", {}).get(\"full_text\", \"\")\n",
    "    # You might also search references, etc.\n",
    "    combined = (title + \" \" + abstract).lower()\n",
    "    return text.lower() in combined\n",
    "def find_matches_in_corpus(text: str, corpus: List[Dict], max_results=2):\n",
    "    \"\"\"\n",
    "    Return a list of up to `max_results` paper indices where `text` is found as substring.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for idx, p in enumerate(corpus):\n",
    "        if is_substring_in_paper(text, p):\n",
    "            matches.append(idx)\n",
    "            if len(matches) >= max_results:\n",
    "                break\n",
    "    return matches\n",
    "verified_memorized = []\n",
    "for suspicious in suspicious_samples:\n",
    "    gen_txt = suspicious[\"completed_sequence\"]\n",
    "    # Try to find this snippet in the local data\n",
    "    matches = find_matches_in_corpus(gen_txt, papers_data, max_results=2)\n",
    "    suspicious[\"corpus_matches\"] = matches\n",
    "    if matches:\n",
    "        verified_memorized.append(suspicious)\n",
    "print(\"[INFO] Verified Memorized Samples (Found in local corpus):\")\n",
    "if not verified_memorized:\n",
    "    print(\" None found. Possibly no direct matches in your subset, or you need more data.\")\n",
    "else:\n",
    "    for vm in verified_memorized:\n",
    "        print(f\" * ratio={vm['zlib_ratio']:.4f} => found in paper indices {vm['corpus_matches']}\")\n",
    "        print(f\"   Prompt: {vm['original_prompt']}\")\n",
    "        print(f\"   Completed: {vm['completed_sequence']}\")\n",
    "        print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Full results saved to membership_inference_results.json. Done.\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# STEP 5: Summarize / Output\n",
    "############################################################################\n",
    "results_file = \"membership_inference_results.json\"\n",
    "with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(extended_samples, f, indent=2)\n",
    "print(f\"[INFO] Full results saved to {results_file}. Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataCon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import zlib\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM as BertModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BIOGPT_MODEL = \"microsoft/BioGPT\"\n",
    "PUBMEDBERT_MODEL = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "PUBMED_DATA = \"./Data/papersNew.json\"\n",
    "PMC_DATA = \"./Data/pubmed_2010_2024_intelligence.json\"\n",
    "OUTPUT_FILE = \"extraction_results.json\"\n",
    "MAX_SAMPLES = 10000  # Total samples to generate\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "biogpt_tokenizer = AutoTokenizer.from_pretrained(BIOGPT_MODEL)\n",
    "biogpt_model = AutoModelForCausalLM.from_pretrained(BIOGPT_MODEL).to(DEVICE)\n",
    "pubmedbert_tokenizer = AutoTokenizer.from_pretrained(PUBMEDBERT_MODEL)\n",
    "pubmedbert_model = BertModel.from_pretrained(PUBMEDBERT_MODEL).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "def load_training_data():\n",
    "    pmc = []\n",
    "\n",
    "    with open(PUBMED_DATA, \"r\", encoding=\"utf-8\") as f:\n",
    "        pubmed = json.load(f)\n",
    "\n",
    "    with open(PMC_DATA, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # skip empty lines\n",
    "                pmc.append(json.loads(line))\n",
    "            \n",
    "    return pubmed, pmc\n",
    "\n",
    "# Preprocess PMC data (ADD THIS STEP)\n",
    "def preprocess_pmc(pmc_data):\n",
    "    processed = []\n",
    "    for entry in pmc_data:\n",
    "        if \"full_text\" in entry and entry[\"full_text\"]:  # Check XML exists\n",
    "            sections = parse_pmc_text(entry[\"full_text\"])\n",
    "            processed.append({\n",
    "                \"pmcid\": entry.get(\"pmc_id\", \"N/A\"),\n",
    "                \"sections\": sections  # Structured sections\n",
    "            })\n",
    "        else:\n",
    "            processed.append({\n",
    "                \"pmcid\": entry.get(\"pmc_id\", \"N/A\"),\n",
    "                \"sections\": {}  # Empty if no content\n",
    "            })\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_data, pmc_data = load_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after filtering (non-empty abstracts): 51568\n"
     ]
    }
   ],
   "source": [
    "# Filter out records with empty abstracts\n",
    "pmc_data = [record for record in pmc_data if record.get(\"abstract\", \"\").strip()]\n",
    "\n",
    "print(f\"Total records after filtering (non-empty abstracts): {len(pmc_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess PMC full text\n",
    "def parse_pmc_text(text):\n",
    "    soup = BeautifulSoup(text, \"xml\")\n",
    "    sections = {}\n",
    "    for sec in soup.find_all(\"sec\"):\n",
    "        title = sec.find(\"title\").get_text(strip=True) if sec.find(\"title\") else \"No Title\"\n",
    "        content = \" \".join(p.get_text(strip=True) for p in sec.find_all(\"p\"))\n",
    "        sections[title] = content\n",
    "    return sections\n",
    "\n",
    "# Generate prefixes from training data\n",
    "def generate_prefixes(data, max_length=10):\n",
    "    prefixes = set()\n",
    "    for entry in data:\n",
    "        if \"abstract\" in entry:\n",
    "            text = entry[\"abstract\"]\n",
    "            sents = re.split(r'(?<=[.!?])\\s+', text)\n",
    "            for sent in sents:\n",
    "                tokens = sent.split()[:max_length]\n",
    "                prefixes.add(\" \".join(tokens) + \"...\")\n",
    "                \n",
    "    return list(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_prefixes = generate_prefixes(pubmed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmc_prefixes = generate_prefixes(pmc_data, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation strategies\n",
    "def generate_with_biogpt(prefix, strategy=\"top_n\", max_length=256):\n",
    "    inputs = biogpt_tokenizer(prefix, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    if strategy == \"top_n\":\n",
    "        outputs = biogpt_model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_k=40,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    elif strategy == \"temperature_decay\":\n",
    "        outputs = biogpt_model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=10.0,\n",
    "            temperature_decay=0.5,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown strategy\")\n",
    "        \n",
    "    return biogpt_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Membership inference metrics\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "def calculate_zlib_entropy(text):\n",
    "    compressed = zlib.compress(text.encode())\n",
    "    return len(compressed) / len(text)\n",
    "\n",
    "def sliding_window_perplexity(text, window_size=50):\n",
    "    min_ppl = float(\"inf\")\n",
    "    for i in range(len(text) - window_size):\n",
    "        window = text[i:i+window_size]\n",
    "        ppl = calculate_perplexity(window, biogpt_model, biogpt_tokenizer)\n",
    "        if ppl < min_ppl:\n",
    "            min_ppl = ppl\n",
    "    return min_ppl\n",
    "\n",
    "# Verification functions\n",
    "def exact_match_check(sample):\n",
    "    for entry in pubmed_data:\n",
    "        if sample in entry[\"abstract\"][\"full_text\"]:\n",
    "            return True\n",
    "    for entry in pmc_data:\n",
    "        if any(sample in sec for sec in entry[\"sections\"].values()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def fuzzy_match_check(sample, threshold=90):\n",
    "    matches = []\n",
    "    for entry in pubmed_data:\n",
    "        score = fuzz.token_set_ratio(sample, entry[\"abstract\"][\"full_text\"])\n",
    "        if score >= threshold:\n",
    "            matches.append(entry[\"pmid\"])\n",
    "    for entry in pmc_data:\n",
    "        for sec in entry[\"sections\"].values():\n",
    "            score = fuzz.token_set_ratio(sample, sec)\n",
    "            if score >= threshold:\n",
    "                matches.append(entry[\"pmcid\"])\n",
    "    return matches\n",
    "\n",
    "# Anonymization\n",
    "def anonymize(text):\n",
    "    # PHI patterns\n",
    "    patterns = {\n",
    "        \"phone\": r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\",\n",
    "        \"email\": r\"[\\w.-]+@[\\w.-]+\",\n",
    "        \"ssn\": r\"\\d{3}-\\d{2}-\\d{4}\",\n",
    "        \"address\": r\"\\d+\\s+[A-Za-z]+\\s+(?:Ave|St|Dr|Ln|Blvd)\\.?\",\n",
    "        \"name\": r\"[A-Z][a-z]+ [A-Z][a-z]+\"\n",
    "    }\n",
    "    \n",
    "    for key, pattern in patterns.items():\n",
    "        text = re.sub(pattern, f\"[{key.upper()} REDACTED]\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [01:40<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 26\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Verification\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m exact_match \u001b[38;5;241m=\u001b[39m exact_match_check(sample)\n\u001b[1;32m     27\u001b[0m fuzzy_matches \u001b[38;5;241m=\u001b[39m fuzzy_match_check(sample)\n",
      "Cell \u001b[0;32mIn[42], line 54\u001b[0m, in \u001b[0;36mexact_match_check\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m pmc_data:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(sample \u001b[38;5;129;01min\u001b[39;00m sec \u001b[38;5;28;01mfor\u001b[39;00m sec \u001b[38;5;129;01min\u001b[39;00m entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msections\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sections'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m prefixes \u001b[38;5;241m=\u001b[39m pubmed_prefixes \u001b[38;5;241m+\u001b[39m pmc_prefixes\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m      5\u001b[0m     futures \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m prefixes[:MAX_SAMPLES]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dataCon/lib/python3.11/concurrent/futures/_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshutdown(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dataCon/lib/python3.11/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         t\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dataCon/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dataCon/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "prefixes = pubmed_prefixes + pmc_prefixes\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for prefix in prefixes[:MAX_SAMPLES]:\n",
    "        futures.append(executor.submit(\n",
    "            generate_with_biogpt,\n",
    "            prefix,\n",
    "            strategy=\"top_n\",\n",
    "            max_length=256\n",
    "        ))\n",
    "        \n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        sample = future.result()\n",
    "        sample = anonymize(sample)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        bio_ppl = calculate_perplexity(sample, biogpt_model, biogpt_tokenizer)\n",
    "        pubmed_ppl = calculate_perplexity(sample, pubmedbert_model, pubmedbert_tokenizer)\n",
    "        ratio = bio_ppl / pubmed_ppl\n",
    "        entropy = calculate_zlib_entropy(sample)\n",
    "        window_ppl = sliding_window_perplexity(sample)\n",
    "        \n",
    "        # Verification\n",
    "        exact_match = exact_match_check(sample)\n",
    "        fuzzy_matches = fuzzy_match_check(sample)\n",
    "        \n",
    "        results.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"sample\": sample,\n",
    "            \"metrics\": {\n",
    "                \"perplexity_ratio\": ratio,\n",
    "                \"zlib_entropy\": entropy,\n",
    "                \"min_window_ppl\": window_ppl\n",
    "            },\n",
    "            \"verification\": {\n",
    "                \"exact_match\": exact_match,\n",
    "                \"fuzzy_matches\": fuzzy_matches\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Save periodically\n",
    "        if len(results) % 100 == 0:\n",
    "            with open(OUTPUT_FILE, \"a\") as f:\n",
    "                for res in results:\n",
    "                    json.dump(res, f)\n",
    "                    f.write(\"\\n\")\n",
    "            results = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataCon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

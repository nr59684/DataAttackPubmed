{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Black?Box Memorization Attack Using PubMedBERT on PubMed Abstracts\n",
    "\n",
    "This script simulates an attack on a domain?specific model in a black?box setting.\n",
    "We load PubMedBERT (a masked LM), generate completions via iterative fill?mask,\n",
    "and then use fuzzy n?gram matching against a local PubMed abstracts corpus (papers.json)\n",
    "to try to detect memorized (verbatim or near-verbatim) sequences.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import zlib\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Configuration\n",
    "###############################################################################\n",
    "MODEL_NAME = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    "CORPUS_JSON_PATH = \"../../Data/papersNew.json\"            # your PubMed abstracts data file\n",
    "OUTPUT_GENERATIONS = \"pubmedbert_generations.json\"\n",
    "ATTACK_RESULTS = \"attack_results_pubmedbert.json\"\n",
    "\n",
    "NUM_GENERATIONS = 2000   # Number of completions to generate\n",
    "MASK_LENGTH = 5          # Number of [MASK] tokens to append and fill\n",
    "TOP_K = 50               # For fill-mask pipeline\n",
    "SUBSTRING_SEARCH_MAX = 2 # Maximum matching articles per candidate\n",
    "FUZZY_N = 2              # Use bigrams\n",
    "FUZZY_THRESHOLD = 0.3    # Fuzzy matching threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Helper Functions\n",
    "###############################################################################\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Lowercase text, remove punctuation, and normalize whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_pubmed_data(json_path: str) -> List[Dict]:\n",
    "    \"\"\"Load PubMed abstracts data from a JSON file.\"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"[ERROR] File not found: {json_path}\")\n",
    "        return []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def zlib_ratio(txt: str) -> float:\n",
    "    \"\"\"Compute zlib compression ratio as: len(text) / len(compressed(text)).\"\"\"\n",
    "    if not txt.strip():\n",
    "        return 0.0\n",
    "    compressed = zlib.compress(txt.encode(\"utf-8\"))\n",
    "    return len(txt) / len(compressed)\n",
    "\n",
    "def fuzzy_ngram_search(snippet: str, corpus: List[Dict], n: int = FUZZY_N, threshold: float = FUZZY_THRESHOLD, max_results: int = SUBSTRING_SEARCH_MAX) -> List[int]:\n",
    "    \"\"\"\n",
    "    Compute n-gram overlap (Jaccard similarity) between the snippet and the combined text\n",
    "    (title + abstract) from each article in the corpus after preprocessing.\n",
    "    \n",
    "    Returns indices of articles where the overlap similarity is at least the threshold.\n",
    "    \"\"\"\n",
    "    snippet = preprocess_text(snippet)\n",
    "    snippet_tokens = snippet.split()\n",
    "    if len(snippet_tokens) < n:\n",
    "        snippet_ngrams = set([tuple(snippet_tokens)])\n",
    "    else:\n",
    "        snippet_ngrams = set(zip(*[snippet_tokens[i:] for i in range(n)]))\n",
    "    \n",
    "    matches = []\n",
    "    for i, article in enumerate(corpus):\n",
    "        title = article.get(\"title\", {}).get(\"full_text\", \"\") or \"\"\n",
    "        abstract = article.get(\"abstract\", {}).get(\"full_text\", \"\") or \"\"\n",
    "        combined = preprocess_text(title + \" \" + abstract)\n",
    "        combined_tokens = combined.split()\n",
    "        if len(combined_tokens) < n:\n",
    "            combined_ngrams = set([tuple(combined_tokens)])\n",
    "        else:\n",
    "            combined_ngrams = set(zip(*[combined_tokens[i:] for i in range(n)]))\n",
    "        \n",
    "        if not snippet_ngrams or not combined_ngrams:\n",
    "            continue\n",
    "\n",
    "        intersection = snippet_ngrams.intersection(combined_ngrams)\n",
    "        union = snippet_ngrams.union(combined_ngrams)\n",
    "        similarity = len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "        if similarity >= threshold:\n",
    "            matches.append(i)\n",
    "            if len(matches) >= max_results:\n",
    "                break\n",
    "\n",
    "    return matches\n",
    "\n",
    "def iterative_fill_mask(prompt: str, num_masks: int, fill_mask_pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Iteratively fill the first [MASK] token in the prompt using the fill-mask pipeline,\n",
    "    replacing it with a randomly selected prediction. This repeats for num_masks iterations.\n",
    "    \"\"\"\n",
    "    sequence = prompt\n",
    "    for _ in range(num_masks):\n",
    "        if \"[MASK]\" not in sequence:\n",
    "            break\n",
    "        first_mask_index = sequence.find(\"[MASK]\")\n",
    "        # Split the sequence to ensure we only fill one mask at a time.\n",
    "        split_once = sequence.split(\"[MASK]\", 1)\n",
    "        text_for_pipeline = split_once[0] + \"[MASK]\" + split_once[1]\n",
    "        results = fill_mask_pipeline(text_for_pipeline)\n",
    "        # results may be a list of lists if multiple masks are detected; we use the first list.\n",
    "        if isinstance(results[0], list):\n",
    "            candidates = results[0]\n",
    "        else:\n",
    "            candidates = results\n",
    "        if not candidates:\n",
    "            break\n",
    "        chosen = random.choice(candidates)\n",
    "        chosen_token = chosen[\"token_str\"]\n",
    "        # Replace only the first occurrence of [MASK]\n",
    "        sequence = sequence[:first_mask_index] + chosen_token + sequence[first_mask_index + len(\"[MASK]\"):]\n",
    "    return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading PubMedBERT model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step A: Load Model & Data ---\n",
    "print(f\"[INFO] Loading PubMedBERT model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading PubMed abstracts data from ../../Data/papersNew.json\n",
      "[INFO] Loaded 157833 records from local PubMed abstracts data.\n"
     ]
    }
   ],
   "source": [
    "# Create a fill-mask pipeline for generation\n",
    "fill_mask_pipe = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer, top_k=TOP_K, device=0 if device==\"cuda\" else -1)\n",
    "\n",
    "print(f\"[INFO] Loading PubMed abstracts data from {CORPUS_JSON_PATH}\")\n",
    "corpus = load_pubmed_data(CORPUS_JSON_PATH)\n",
    "print(f\"[INFO] Loaded {len(corpus)} records from local PubMed abstracts data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Built 305510 candidate prompts.\n"
     ]
    }
   ],
   "source": [
    "# --- Step B: Build Candidate Prompts ---\n",
    "# We use title full_text and first 20 words of abstract as prompts.\n",
    "prompts = []\n",
    "for article in corpus:\n",
    "    title_text = article.get(\"title\", {}).get(\"full_text\", \"\").strip()\n",
    "    abstract_text = article.get(\"abstract\", {}).get(\"full_text\", \"\").strip()\n",
    "    if title_text:\n",
    "        prompts.append(title_text)\n",
    "    if abstract_text:\n",
    "        words = abstract_text.split()\n",
    "        prompt_abstract = \" \".join(words[:20]) if len(words) > 20 else abstract_text\n",
    "        prompts.append(prompt_abstract)\n",
    "if not prompts:\n",
    "    prompts = [\"Biomedical research shows\", \"In this study, we explore\"]\n",
    "print(f\"[INFO] Built {len(prompts)} candidate prompts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generated 100 completions.\n",
      "[INFO] Generated 200 completions.\n",
      "[INFO] Generated 300 completions.\n",
      "[INFO] Generated 400 completions.\n",
      "[INFO] Generated 500 completions.\n",
      "[INFO] Generated 600 completions.\n",
      "[INFO] Generated 700 completions.\n",
      "[INFO] Generated 800 completions.\n",
      "[INFO] Generated 900 completions.\n",
      "[INFO] Generated 1000 completions.\n",
      "[INFO] Generated 1100 completions.\n",
      "[INFO] Generated 1200 completions.\n",
      "[INFO] Generated 1300 completions.\n",
      "[INFO] Generated 1400 completions.\n",
      "[INFO] Generated 1500 completions.\n",
      "[INFO] Generated 1600 completions.\n",
      "[INFO] Generated 1700 completions.\n",
      "[INFO] Generated 1800 completions.\n",
      "[INFO] Generated 1900 completions.\n",
      "[INFO] Generated 2000 completions.\n",
      "[INFO] Saved 2000 completions to pubmedbert_generations.json.\n"
     ]
    }
   ],
   "source": [
    "# --- Step C: Generate Completions ---\n",
    "# For each generation, choose a random prompt, append a sequence of [MASK] tokens, and fill them iteratively.\n",
    "generations = []\n",
    "for i in range(NUM_GENERATIONS):\n",
    "    prompt = random.choice(prompts)\n",
    "    masked_prompt = prompt + \" \" + \" \".join([\"[MASK]\"] * MASK_LENGTH)\n",
    "    generated_text = iterative_fill_mask(masked_prompt, MASK_LENGTH, fill_mask_pipe)\n",
    "    ratio = zlib_ratio(generated_text)\n",
    "    generations.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"zlib_ratio\": ratio\n",
    "    })\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"[INFO] Generated {i+1} completions.\")\n",
    "\n",
    "with open(OUTPUT_GENERATIONS, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(generations, f, indent=2)\n",
    "print(f\"[INFO] Saved {len(generations)} completions to {OUTPUT_GENERATIONS}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Top 5 suspicious completions by zlib_ratio:\n",
      "1. zlib_ratio: 1.7500\n",
      "Prompt: in this work, the interaction mechanisms between an autotrophic denitrification (ad) and heterotrophic denitrification (hd) process in a heterotrophic-autotrophic denitrification\n",
      "Generated (first 150 chars): in this work, the interaction mechanisms between an autotrophic denitrification (ad) and heterotrophic denitrification (hd) process in a heterotrophic...\n",
      "------------------------------------------------------------\n",
      "2. zlib_ratio: 1.6357\n",
      "Prompt: this study investigated age differences in appetitive and aversive associative learning using a pavlovian conditioning paradigm. appetitive and aversive associative\n",
      "Generated (first 150 chars): this study investigated age differences in appetitive and aversive associative learning using a pavlovian conditioning paradigm. appetitive and aversi...\n",
      "------------------------------------------------------------\n",
      "3. zlib_ratio: 1.6260\n",
      "Prompt: micrornas (mirnas) are small non-coding rnas that modulate gene expression transcriptionally (transcriptional activation or inactivation) and/or post-transcriptionally (translation inhibition or\n",
      "Generated (first 150 chars): micrornas (mirnas) are small non-coding rnas that modulate gene expression transcriptionally (transcriptional activation or inactivation) and/or post-...\n",
      "------------------------------------------------------------\n",
      "4. zlib_ratio: 1.6154\n",
      "Prompt: intelligence differences of individuals are attributed to the structural and functional differences of the brain. neural processing operations of the\n",
      "Generated (first 150 chars): intelligence differences of individuals are attributed to the structural and functional differences of the brain. neural processing operations of the ...\n",
      "------------------------------------------------------------\n",
      "5. zlib_ratio: 1.6016\n",
      "Prompt: we examined three communication ability classification paradigms for children with cerebral palsy (cp): the communication function classification system (cfcs), the\n",
      "Generated (first 150 chars): we examined three communication ability classification paradigms for children with cerebral palsy (cp): the communication function classification syst...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Step D: Membership Inference Filtering ---\n",
    "# Sort generations by zlib_ratio (higher might be suspicious) and pick the top 50.\n",
    "generations.sort(key=lambda x: x[\"zlib_ratio\"], reverse=True)\n",
    "top_suspicious = generations[:50]\n",
    "print(\"[INFO] Top 5 suspicious completions by zlib_ratio:\")\n",
    "for j, cand in enumerate(top_suspicious[:5], start=1):\n",
    "    print(f\"{j}. zlib_ratio: {cand['zlib_ratio']:.4f}\")\n",
    "    print(f\"Prompt: {cand['prompt']}\")\n",
    "    print(f\"Generated (first 150 chars): {cand['generated_text'][:150]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiple studies successfully applied multivariate analysis to neuroimaging data demonstrating the potential utility of neuroimaging for clinical diagnostic and prognostic purposes.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1538]['abstract']['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_generated_portion(prompt: str, generated: str) -> str:\n",
    "    \"\"\"\n",
    "    If the generated text starts with the prompt, return only the portion\n",
    "    after the prompt. Otherwise, return the full generated text.\n",
    "    \"\"\"\n",
    "    if generated.startswith(prompt):\n",
    "        return generated[len(prompt):].strip()\n",
    "    return generated.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Verified memorized samples (fuzzy matching): 0\n",
      "[INFO] Attack results saved to attack_results_pubmedbert.json\n"
     ]
    }
   ],
   "source": [
    "# --- Step E: Verification via Fuzzy n-gram Matching ---\n",
    "verified_memorized = []\n",
    "for suspicious in top_suspicious:\n",
    "    # Extract just the generated (filled-mask) part\n",
    "    gen_portion = extract_generated_portion(suspicious[\"prompt\"], suspicious[\"generated_text\"])\n",
    "    matches = fuzzy_ngram_search(gen_portion, corpus, n=FUZZY_N, threshold=FUZZY_THRESHOLD, max_results=SUBSTRING_SEARCH_MAX)\n",
    "    if matches:\n",
    "        suspicious[\"matches\"] = matches\n",
    "        verified_memorized.append(suspicious)\n",
    "\n",
    "print(f\"[INFO] Verified memorized samples (fuzzy matching): {len(verified_memorized)}\")\n",
    "for vm in verified_memorized:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"zlib_ratio: {vm['zlib_ratio']:.4f}\")\n",
    "    print(f\"Prompt: {vm['prompt']}\")\n",
    "    print(f\"Generated Text: {vm['generated_text']}\")\n",
    "    print(f\"Found in corpus indices: {vm['matches']}\")\n",
    "\n",
    "results = {\n",
    "    \"generations\": generations,\n",
    "    \"verified_memorized\": verified_memorized\n",
    "}\n",
    "with open(ATTACK_RESULTS, \"w\", encoding=\"utf-8\") as rf:\n",
    "    json.dump(results, rf, indent=2)\n",
    "print(f\"[INFO] Attack results saved to {ATTACK_RESULTS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataCon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97ec761001ea25a843739c998b516e9c906daf472aa6d0129e56e3aea5e0f0e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

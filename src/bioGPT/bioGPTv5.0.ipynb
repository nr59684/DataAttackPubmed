{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Final Integrated Pipeline for PubMed Domain LLM Memorization Attack\n",
    "\n",
    "Fixes common pitfalls:\n",
    "  - Uses partial real prefixes from actual pubmed data (pubmedPapers.json).\n",
    "  - Generates long completions with enough sampling (30k or more).\n",
    "  - Applies a 50-token membership check (suffix array or fuzzy search).\n",
    "  - Includes single-token \"divergence\" prompts (if using an auto-regressive model like BioGPT).\n",
    "\n",
    "If no memorization is found, it logs stats (like average length of completions) to see what might be going wrong.\n",
    "\n",
    "Authors: Nilesh Rijhwani & Bhavana Krishna\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    "    # or AutoModelForMaskedLM and pipeline if using masked LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "PUBMED_JSON_PATH = \"../../Data/papersNew.json\"  # your local pubmed data in the structure you provided\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Domain LLM (BioGPT as example). If you want masked LM, adapt below.\n",
    "MODEL_NAME = \"microsoft/BioGPT-Large\"\n",
    "MODEL_TYPE = \"auto-regressive\"  # or \"masked-lm\"\n",
    "\n",
    "# Large scale generation\n",
    "NUM_GENERATIONS = 30000       # adjust higher if you can\n",
    "MAX_NEW_TOKENS = 512          # encourage longer completions\n",
    "TEMPERATURE = 1.2             # slightly higher than default, fosters memorization\n",
    "TOP_K = 50\n",
    "TOP_P = 0.9\n",
    "\n",
    "# Minimum tokens for a \"real prefix\" (we only want lines with at least 40 tokens)\n",
    "MIN_TOKENS_PREFIX = 40\n",
    "REAL_PREFIX_LIMIT = 1000  # how many real lines to store as candidate prefixes\n",
    "\n",
    "# Divergence single tokens (for auto-reg LMs)\n",
    "SINGLE_TOKEN_LIST = [\"gene\", \"cells\", \"protein\", \"analysis\", \"dna\"]\n",
    "\n",
    "# Random domain seeds\n",
    "DOMAIN_SEEDS = [\n",
    "    \"The disease outbreak was caused by\",\n",
    "    \"We discovered new gene expression patterns in\",\n",
    "    \"Using CRISPR, we tested the effect on cell lines\",\n",
    "    \"The study had a p-value of\"\n",
    "]\n",
    "\n",
    "# Minimum length for membership check (e.g. 50 tokens)\n",
    "MIN_MEMORIZATION_TOKENS = 50\n",
    "\n",
    "# Suffix array or fuzzy approach\n",
    "USE_SUFFIX_ARRAY = True\n",
    "SUFFIX_ARRAY_PATH = \"/path/to/pubmed_suffix_array.bin\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_GENERATIONS_FILE = \"pubmed_generations.json\"\n",
    "OUTPUT_MEMORIZED_FILE   = \"pubmed_memorized.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2. LOADING pubmedPapers.json & GATHERING REAL PREFIXES\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def gather_real_prefixes_from_pubmed(json_path, max_samples=1000, min_tokens=40):\n",
    "    \"\"\"\n",
    "    Load pubmedPapers.json and gather lines from the abstracts \n",
    "    that have >= min_tokens. We'll store the 'full_text' as a single line.\n",
    "    Return up to max_samples random lines.\n",
    "    \"\"\"\n",
    "    all_lines = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # expects a list of items\n",
    "\n",
    "    for item in data:\n",
    "        # item structure as user described\n",
    "        # we want the 'abstract' -> 'full_text' if it has enough tokens\n",
    "        abstr = item.get(\"abstract\", {})\n",
    "        text = abstr.get(\"full_text\", \"\")\n",
    "        tokens = abstr.get(\"tokens\", [])\n",
    "        if len(tokens) >= min_tokens:\n",
    "            all_lines.append(text.strip())\n",
    "\n",
    "        # Optionally, you could also consider the 'title' if it?s big enough:\n",
    "        # title_dict = item.get(\"title\", {})\n",
    "        # ttext = title_dict.get(\"full_text\", \"\")\n",
    "        # if len(title_dict.get(\"tokens\", [])) >= min_tokens:\n",
    "        #     all_lines.append(ttext.strip())\n",
    "\n",
    "    random.shuffle(all_lines)\n",
    "    return all_lines[:max_samples]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. GENERATION HELPERS\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def generate_ar_text(model, tokenizer, prompt):\n",
    "    \"\"\"Auto-regressive generation for e.g. BioGPT.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_k=TOP_K,\n",
    "            top_p=TOP_P,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )[0]\n",
    "    return tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "\n",
    "def generate_single_token_divergence(model, tokenizer, token=\"gene\"):\n",
    "    \"\"\"\n",
    "    Repeated single-token prompt to attempt 'divergence' for auto-reg LMs like BioGPT.\n",
    "    \"\"\"\n",
    "    repeated_prompt = (token + \" \") * 300  # repeat it 300 times\n",
    "    return generate_ar_text(model, tokenizer, repeated_prompt)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. MEMBERSHIP CHECK\n",
    "#    For demonstration, we show a suffix_array approach \n",
    "#    and a stub for fuzzy approach.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def check_membership_suffixarray(text, min_len=50):\n",
    "    \"\"\"\n",
    "    Pseudocode: do a sliding window of 50 tokens \n",
    "    or pass the entire text to your suffix search if it can handle it.\n",
    "    \"\"\"\n",
    "    from suffix_array_tool import suffix_search  # user-provided\n",
    "    found = suffix_search(text, SUFFIX_ARRAY_PATH, min_len_tokens=min_len)\n",
    "    return found\n",
    "\n",
    "def check_membership_fuzzy(text, min_len=50):\n",
    "    \"\"\"Placeholder for fuzzy n-gram approach if suffix array yields zero hits.\"\"\"\n",
    "    return False  # implement if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load domain model\n",
    "print(f\"[INFO] Loading model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "# Gather partial real prefixes from pubmedPapers.json\n",
    "real_prefixes = gather_real_prefixes_from_pubmed(\n",
    "    PUBMED_JSON_PATH,\n",
    "    max_samples=REAL_PREFIX_LIMIT,\n",
    "    min_tokens=MIN_TOKENS_PREFIX\n",
    ")\n",
    "print(f\"[INFO] Found {len(real_prefixes)} real domain lines (prefix candidates).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_generations = []\n",
    "i = 0\n",
    "while i < NUM_GENERATIONS:\n",
    "    # 1) partial-real prefix\n",
    "    if real_prefixes:\n",
    "        prefix = random.choice(real_prefixes)\n",
    "    else:\n",
    "        prefix = random.choice(DOMAIN_SEEDS)\n",
    "    gen_text = generate_ar_text(model, tokenizer, prefix)\n",
    "    all_generations.append({\n",
    "        \"prompt\": prefix,\n",
    "        \"generated_text\": gen_text,\n",
    "        \"method\": \"partial-real\"\n",
    "    })\n",
    "    i += 1\n",
    "    if i >= NUM_GENERATIONS:\n",
    "        break\n",
    "    # 2) single-token approach\n",
    "    single_tok = random.choice(SINGLE_TOKEN_LIST)\n",
    "    div_text = generate_single_token_divergence(model, tokenizer, token=single_tok)\n",
    "    all_generations.append({\n",
    "        \"prompt\": single_tok,\n",
    "        \"generated_text\": div_text,\n",
    "        \"method\": \"divergence\"\n",
    "    })\n",
    "    i += 1\n",
    "    if i >= NUM_GENERATIONS:\n",
    "        break\n",
    "    # 3) random domain approach\n",
    "    domain_seed = random.choice(DOMAIN_SEEDS)\n",
    "    r_text = generate_ar_text(model, tokenizer, domain_seed)\n",
    "    all_generations.append({\n",
    "        \"prompt\": domain_seed,\n",
    "        \"generated_text\": r_text,\n",
    "        \"method\": \"domain-seed\"\n",
    "    })\n",
    "    i += 1\n",
    "# Save raw completions\n",
    "print(f\"[INFO] Generated {len(all_generations)} completions. Saving to {OUTPUT_GENERATIONS_FILE}\")\n",
    "with open(OUTPUT_GENERATIONS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_generations, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membership check\n",
    "memorized = []\n",
    "found_cnt = 0\n",
    "for entry in all_generations:\n",
    "    txt = entry[\"generated_text\"]\n",
    "    found = False\n",
    "    if USE_SUFFIX_ARRAY:\n",
    "        found = check_membership_suffixarray(txt, min_len=MIN_MEMORIZATION_TOKENS)\n",
    "    else:\n",
    "        found = check_membership_fuzzy(txt, min_len=MIN_MEMORIZATION_TOKENS)\n",
    "    if found:\n",
    "        memorized.append(entry)\n",
    "        found_cnt += 1\n",
    "print(f\"[RESULT] Found {found_cnt} memorized completions out of {len(all_generations)}.\")\n",
    "with open(OUTPUT_MEMORIZED_FILE, \"w\", encoding=\"utf-8\") as mf:\n",
    "    json.dump(memorized, mf, indent=2)\n",
    "if found_cnt == 0:\n",
    "    # Additional analysis\n",
    "    print(\"[WARN] Zero memorized strings found.\")\n",
    "    method_counts = Counter(e[\"method\"] for e in all_generations)\n",
    "    print(\"Method usage stats:\", method_counts)\n",
    "    lengths = [len(e[\"generated_text\"].split()) for e in all_generations]\n",
    "    avg_len = sum(lengths)/len(lengths) if lengths else 0\n",
    "    print(f\"Average generation length: {avg_len:.2f} tokens\")\n",
    "    plt.hist(lengths, bins=50)\n",
    "    plt.title(\"Generation Output Length Distribution\")\n",
    "    plt.xlabel(\"Tokens per generation\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(\"pubmed_generation_lengths.png\")\n",
    "    plt.close()\n",
    "    print(\"Suggestions if truly zero:\")\n",
    "    print(\"- Increase NUM_GENERATIONS further (e.g. 100k or more).\")\n",
    "    print(\"- Increase max_new_tokens (e.g. 512 -> 1024).\")\n",
    "    print(\"- Verify you have the same snapshot of data. If data is mismatched, membership checks won't find hits.\")\n",
    "    print(\"- Possibly use fuzzy approach in case small differences appear.\")\n",
    "else:\n",
    "    print(f\"[INFO] Memorized samples saved to {OUTPUT_MEMORIZED_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataCon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97ec761001ea25a843739c998b516e9c906daf472aa6d0129e56e3aea5e0f0e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full Pipeline: BioGPT Data Extraction Example\n",
    "\n",
    "This script demonstrates:\n",
    "  1. Loading BioGPT (a large language model).\n",
    "  2. Loading a local subset of PubMed data (papers.json).\n",
    "  3. Generating prompts with first 20 words from abstract.\n",
    "  4. Generating cpmpletions from BioGPT without any sampling or black-box approach.\n",
    "  5. Applying naive approach- calculation zlib ratio for all the generations and selecting top-50.\n",
    "  5. Searching the local PubMed data to see if the text is indeed memorized\n",
    "     (naive_substring_search).\n",
    "\n",
    "DISCLAIMER:\n",
    "  - The membership inference here is simplified. Real approaches might\n",
    "    compare perplexities from multiple models or do more advanced metrics.\n",
    "  - The substring search is naive and may need optimization or fuzzy matching.\n",
    "  - This code is a proof-of-concept. We will Modify and expand to suit our needs.\n",
    "\n",
    "Authors: Nilesh Rijhwani and Bhavana Krishna\n",
    "\n",
    "Version: 1.0\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import zlib\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Configuration\n",
    "###############################################################################\n",
    "MODEL_NAME = \"microsoft/BioGPT\"  # Or \"microsoft/BioGPT\"\n",
    "PMC_JSON_PATH = \"./Data/pubmed_2010_2024_intelligence.json\"   # file with your downloaded PMC data\n",
    "OUTPUT_GENERATIONS = \"biogpt_generations.json\"\n",
    "ATTACK_RESULTS = \"attack_results.json\"\n",
    "\n",
    "NUM_GENERATIONS = 15000  # total completions to generate\n",
    "TOKENS_TO_GENERATE = 512  # tokens each time\n",
    "TOP_K = 50\n",
    "TOP_P = 0.95\n",
    "TEMPERATURE = 0.8\n",
    "SUBSTRING_SEARCH_MAX = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Functions\n",
    "###############################################################################\n",
    "\n",
    "def load_pmc_data(json_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Loads the local PMC data from a JSON file.\n",
    "    Expects a list of dicts each with 'full_text', 'abstract', etc.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"[ERROR] PMC data file not found: {json_path}\")\n",
    "        return []\n",
    "    data=[]\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # skip empty lines\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def zlib_ratio(txt: str) -> float:\n",
    "    \"\"\"\n",
    "    Naive measure of how well the text compresses.\n",
    "    ratio = len(txt) / len(zlib.compress(txt.encode('utf-8')))\n",
    "    Lower ratio might indicate random/unstructured text\n",
    "    or higher ratio might indicate repeated structure.\n",
    "    \"\"\"\n",
    "    if not txt.strip():\n",
    "        return 0.0\n",
    "    c = zlib.compress(txt.encode(\"utf-8\"))\n",
    "    return len(txt) / len(c)\n",
    "\n",
    "def naive_substring_search(snippet: str, corpus: List[Dict], max_results: int = 2) -> List[int]:\n",
    "    \"\"\"\n",
    "    Check if 'snippet' appears verbatim in the 'full_text' or 'abstract' or 'title'\n",
    "    for each article in the corpus. Return indices of up to max_results matches.\n",
    "\n",
    "    Note: Very naive substring match, ignoring punctuation/casing differences.\n",
    "    \"\"\"\n",
    "    snippet_lower = snippet.lower()\n",
    "    matches = []\n",
    "    for i, article in enumerate(corpus):\n",
    "        # combine multiple fields\n",
    "        combined_text = (\n",
    "            (article.get(\"title\", \"\") + \" \") +\n",
    "            (article.get(\"abstract\", \"\") + \" \") +\n",
    "            (article.get(\"full_text\") or \"\")\n",
    "        ).lower()\n",
    "        if snippet_lower in combined_text:\n",
    "            matches.append(i)\n",
    "            if len(matches) >= max_results:\n",
    "                break\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading BioGPT model: microsoft/BioGPT\n",
      "[INFO] Loading local PMC data from ./Data/pubmed_2010_2024_intelligence.json\n",
      "[INFO] Found 51568 articles in local corpus.\n",
      "[INFO] Built 103136 candidate seed prompts from the data.\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Step A: Load Model & Data\n",
    "############################################################################\n",
    "print(f\"[INFO] Loading BioGPT model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"[INFO] Loading local PMC data from {PMC_JSON_PATH}\")\n",
    "pmc_data = load_pmc_data(PMC_JSON_PATH)\n",
    "pmc_data = [record for record in pmc_data if record.get(\"abstract\", \"\").strip()]\n",
    "print(f\"[INFO] Found {len(pmc_data)} articles in local corpus.\")\n",
    "# We'll create a small list of \"seed prompts\" from the PMC data\n",
    "# e.g., random lines from 'title' or 'full_text'\n",
    "# We'll store them in a list of strings\n",
    "prompts = []\n",
    "for article in pmc_data:\n",
    "    t = article.get(\"title\", \"\").strip()\n",
    "    if t:\n",
    "        prompts.append(t)\n",
    "    ab = article.get(\"abstract\", \"\").strip()\n",
    "    if ab:\n",
    "        # maybe take first 20 words\n",
    "        words = ab.split()\n",
    "        partial_ab = \" \".join(words[:20])\n",
    "        prompts.append(partial_ab)\n",
    "if not prompts:\n",
    "    prompts = [\"Biomedical research indicates\", \"In this study, we explore\"]  # fallback\n",
    "print(f\"[INFO] Built {len(prompts)} candidate seed prompts from the data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generated 15000 completions total.\n",
      "[INFO] Wrote raw generations to biogpt_generations.json.\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Step B: Generate Text\n",
    "############################################################################\n",
    "# We'll do a simple approach: pick random prompts from 'prompts' and generate\n",
    "# completions from BioGPT. Then store them in a list.\n",
    "all_generations = []\n",
    "for _ in range(NUM_GENERATIONS):\n",
    "    # pick random prompt\n",
    "    prompt = random.choice(prompts)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_seq = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=TOKENS_TO_GENERATE,\n",
    "            do_sample=True,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_k=TOP_K,\n",
    "            top_p=TOP_P\n",
    "        )[0]\n",
    "    generated_text = tokenizer.decode(output_seq, skip_special_tokens=True)\n",
    "    all_generations.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_text\": generated_text\n",
    "    })\n",
    "print(f\"[INFO] Generated {len(all_generations)} completions total.\")\n",
    "# Save raw generations\n",
    "with open(OUTPUT_GENERATIONS, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_generations, f, indent=2)\n",
    "print(f\"[INFO] Wrote raw generations to {OUTPUT_GENERATIONS}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Top 50 suspicious by zlib ratio (descending):\n",
      "1. ratio=2.5192 => To summarize recently published key articles on the topics of biomedical engineering, biotechnology ...\n",
      "2. ratio=2.2910 => Insomnia, intelligence and neuroticism are three typical traits and dysfunctions mainly regulated by...\n",
      "3. ratio=2.2821 => To explore the cross-level relationships between group organisational citizenship behaviour, emotion...\n",
      "4. ratio=2.1987 => We demonstrate a link between preschoolers' quantitative competencies and their school-entry knowled...\n",
      "5. ratio=2.1543 => In this piece, Daniel Leufer introduces his project, aimyths.org, a website that tackles eight of th...\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Step C: Membership Inference (zlib ratio)\n",
    "############################################################################\n",
    "# Let's do a naive approach: compute zlib_ratio for each generation. \n",
    "# Then pick the top-50 or top-100 \"most suspicious\" (lowest or highest ratio?).\n",
    "# Typically, \"lowest perplexity\" => \"lowest ratio\" might be suspicious, \n",
    "# but this is very heuristic. We'll just pick the top 50 with the \"highest ratio\"\n",
    "# to demonstrate. (Carlini's approach can vary.)\n",
    "# Let's define \"highest ratio\" as suspicious (lots of repeated patterns => bigger compress).\n",
    "# Alternatively, do \"lowest ratio\" if you interpret random text compresses worse. \n",
    "# You can experiment either direction.\n",
    "extended_gens = []\n",
    "for g in all_generations:\n",
    "    txt = g[\"generated_text\"]\n",
    "    ratio = zlib_ratio(txt)\n",
    "    g[\"zlib_ratio\"] = ratio\n",
    "    extended_gens.append(g)\n",
    "# Sort by ratio descending (could do ascending if you prefer).\n",
    "extended_gens.sort(key=lambda x: x[\"zlib_ratio\"], reverse=True)\n",
    "# Let's pick top 50 as suspicious\n",
    "top_suspicious = extended_gens[:50]\n",
    "print(\"[INFO] Top 50 suspicious by zlib ratio (descending):\")\n",
    "for i, sus in enumerate(top_suspicious[:5], start=1):\n",
    "    print(f\"{i}. ratio={sus['zlib_ratio']:.4f} => {sus['generated_text'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 0 'verified' memorized samples (exact substring).\n",
      "[INFO] Attack complete. Results in attack_results.json.\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Step D: Verify by Searching in Local Data\n",
    "############################################################################\n",
    "verified_memorized = []\n",
    "for sus in top_suspicious:\n",
    "    snippet = sus[\"generated_text\"]\n",
    "    matches = naive_substring_search(snippet, pmc_data, max_results=SUBSTRING_SEARCH_MAX)\n",
    "    if matches:\n",
    "        sus[\"corpus_matches\"] = matches\n",
    "        verified_memorized.append(sus)\n",
    "print(f\"[INFO] Found {len(verified_memorized)} 'verified' memorized samples (exact substring).\")\n",
    "for vm in verified_memorized:\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(f\"zlib_ratio={vm['zlib_ratio']:.4f}\")\n",
    "    print(f\"Prompt: {vm['prompt']}\")\n",
    "    print(f\"Generated: {vm['generated_text']}\")\n",
    "    print(f\"Matches in corpus indices: {vm['corpus_matches']}\")\n",
    "# Save final results\n",
    "results_dict = {\n",
    "    \"all_generations\": extended_gens,\n",
    "    \"verified_memorized\": verified_memorized\n",
    "}\n",
    "with open(ATTACK_RESULTS, \"w\", encoding=\"utf-8\") as rf:\n",
    "    json.dump(results_dict, rf, indent=2)\n",
    "print(f\"[INFO] Attack complete. Results in {ATTACK_RESULTS}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataCon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "97ec761001ea25a843739c998b516e9c906daf472aa6d0129e56e3aea5e0f0e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

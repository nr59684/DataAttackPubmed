{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Refined Black-Box Memorization Attack for BioGPT:\n",
    "- Auto-regressive text generation\n",
    "- Sliding-window perplexity analysis\n",
    "- Embedding-based similarity (BioBERT-NLI) for near-verbatim detection\n",
    "\n",
    "Steps:\n",
    "  1. Loads BioGPT as a black-box model (only using model.generate, plus a minimal forward pass for perplexity).\n",
    "  2. Loads a local PubMed/PMC corpus from papers.json (with 'title' and 'abstract' fields).\n",
    "  3. Builds candidate prompts from titles and partial abstracts.\n",
    "  4. Generates text with top-k/nucleus sampling.\n",
    "  5. Computes:\n",
    "       - zlib ratio (a naive compression-based signal)\n",
    "       - sliding-window perplexity across the generated output\n",
    "  6. Uses domain-specific embeddings to compare each generated completion to the corpus.\n",
    "  7. Prints and saves suspicious results.\n",
    "\n",
    "Requires:\n",
    "  - transformers\n",
    "  - torch\n",
    "  - sentence-transformers\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import zlib\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CONFIGURATION\n",
    "###############################################################################\n",
    "BIOGPT_MODEL_NAME = \"microsoft/BioGPT-Large\"  # Or \"microsoft/BioGPT\"\n",
    "CORPUS_JSON_PATH = \"papers.json\"             # local PubMed/PMC dataset\n",
    "GENERATIONS_FILE = \"biogpt_generations.json\"\n",
    "ATTACK_RESULTS_FILE = \"biogpt_attack_results.json\"\n",
    "\n",
    "NUM_GENERATIONS = 1000    # number of completions to generate\n",
    "TOKENS_TO_GENERATE = 200  # length of each generated text\n",
    "TEMPERATURE = 0.8\n",
    "TOP_K = 50\n",
    "TOP_P = 0.95\n",
    "\n",
    "WINDOW_SIZE = 50          # sliding window size for perplexity\n",
    "STRIDE_FRACTION = 0.5     # overlap fraction for sliding window (e.g. 0.5 => half overlap)\n",
    "\n",
    "EMB_MODEL_NAME = \"pritamdeka/BioBERT-NLI-mean-tokens\"  # domain-specific embedding model\n",
    "EMB_SIM_THRESHOLD = 0.85   # similarity threshold to consider near-verbatim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Helper Functions\n",
    "###############################################################################\n",
    "def load_pubmed_data(json_path: str) -> List[Dict]:\n",
    "    \"\"\"Load the PubMed abstracts data from a JSON file.\"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"[ERROR] File not found: {json_path}\")\n",
    "        return []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Lowercase, remove punctuation, and normalize whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def zlib_ratio(txt: str) -> float:\n",
    "    \"\"\"Compute zlib compression ratio as a membership inference metric.\"\"\"\n",
    "    if not txt.strip():\n",
    "        return 0.0\n",
    "    compressed = zlib.compress(txt.encode(\"utf-8\"))\n",
    "    return len(txt) / len(compressed)\n",
    "\n",
    "def sliding_window_perplexity(text: str, model, tokenizer, device: str, window_size: int = WINDOW_SIZE) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute perplexity for overlapping sliding windows of size 'window_size'.\n",
    "    Returns the minimum perplexity and the average perplexity across windows.\n",
    "    \"\"\"\n",
    "    # Tokenize text (we assume text is a string)\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids[0]  # shape: (seq_len,)\n",
    "    seq_len = input_ids.size(0)\n",
    "    if seq_len < window_size:\n",
    "        window_size = seq_len\n",
    "    window_perplexities = []\n",
    "    # Slide window over the sequence (with a stride, e.g., half the window size)\n",
    "    stride = window_size // 2 if window_size > 1 else 1\n",
    "    for i in range(0, seq_len - window_size + 1, stride):\n",
    "        window_ids = input_ids[i:i+window_size].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(window_ids, labels=window_ids)\n",
    "        loss = outputs.loss\n",
    "        ppl = torch.exp(loss).item()\n",
    "        window_perplexities.append(ppl)\n",
    "    if not window_perplexities:\n",
    "        return {\"min_ppl\": None, \"avg_ppl\": None}\n",
    "    return {\"min_ppl\": min(window_perplexities), \"avg_ppl\": sum(window_perplexities)/len(window_perplexities)}\n",
    "\n",
    "def embedding_similarity(generated_text: str, corpus: List[Dict], emb_model, field: str = \"title_abstract\") -> List[int]:\n",
    "    \"\"\"\n",
    "    Compute embedding similarity between the generated text and each article in the corpus.\n",
    "    For each article, we combine title and abstract fields (preprocessed).\n",
    "    Returns indices of articles where cosine similarity is above EMB_SIM_THRESHOLD.\n",
    "    \"\"\"\n",
    "    gen_emb = emb_model.encode(generated_text, convert_to_tensor=True)\n",
    "    indices = []\n",
    "    corpus_texts = []\n",
    "    for article in corpus:\n",
    "        title = article.get(\"title\", {}).get(\"full_text\", \"\") or \"\"\n",
    "        abstract = article.get(\"abstract\", {}).get(\"full_text\", \"\") or \"\"\n",
    "        combined = preprocess_text(title + \" \" + abstract)\n",
    "        corpus_texts.append(combined)\n",
    "    corpus_embs = emb_model.encode(corpus_texts, convert_to_tensor=True)\n",
    "    cos_scores = util.cos_sim(gen_emb, corpus_embs)[0]  # 1D tensor of similarity scores\n",
    "    for idx, score in enumerate(cos_scores):\n",
    "        if score.item() >= EMB_SIM_THRESHOLD:\n",
    "            indices.append(idx)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step A: Load Model & Data ---\n",
    "print(f\"[INFO] Loading PubMedBERT model (masked LM): {BIOGPT_MODEL}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BIOGPT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(BIOGPT_MODEL)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(f\"[INFO] Loading PubMed abstracts data from {CORPUS_JSON_PATH}\")\n",
    "corpus = load_pubmed_data(CORPUS_JSON_PATH)\n",
    "print(f\"[INFO] Loaded {len(corpus)} records from local PubMed abstracts.\")\n",
    "\n",
    "# Build candidate prompts from titles and first 20 words of abstracts\n",
    "prompts = []\n",
    "for article in corpus:\n",
    "    title = article.get(\"title\", {}).get(\"full_text\", \"\").strip()\n",
    "    abstract = article.get(\"abstract\", {}).get(\"full_text\", \"\").strip()\n",
    "    if title:\n",
    "        prompts.append(title)\n",
    "    if abstract:\n",
    "        words = abstract.split()\n",
    "        prompt_abstract = \" \".join(words[:20]) if len(words) > 20 else abstract\n",
    "        prompts.append(prompt_abstract)\n",
    "if not prompts:\n",
    "    prompts = [\"Biomedical research shows\", \"In this study, we explore\"]\n",
    "print(f\"[INFO] Built {len(prompts)} candidate prompts.\")\n",
    "\n",
    "# --- Step B: Generate Completions ---\n",
    "generations = []\n",
    "# For PubMedBERT, we simulate generation using an iterative fill-mask approach.\n",
    "# Create a fill-mask pipeline for the model.\n",
    "fill_mask_pipe = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer, top_k=TOP_K, device=0 if device==\"cuda\" else -1)\n",
    "\n",
    "def iterative_fill_mask(prompt: str, num_masks: int, fill_mask_pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Iteratively fill the first [MASK] token in the prompt using the fill-mask pipeline.\n",
    "    \"\"\"\n",
    "    sequence = prompt\n",
    "    for _ in range(num_masks):\n",
    "        if \"[MASK]\" not in sequence:\n",
    "            break\n",
    "        first_mask_index = sequence.find(\"[MASK]\")\n",
    "        split_once = sequence.split(\"[MASK]\", 1)\n",
    "        text_for_pipeline = split_once[0] + \"[MASK]\" + split_once[1]\n",
    "        results = fill_mask_pipeline(text_for_pipeline)\n",
    "        if isinstance(results[0], list):\n",
    "            candidates = results[0]\n",
    "        else:\n",
    "            candidates = results\n",
    "        if not candidates:\n",
    "            break\n",
    "        chosen = random.choice(candidates)\n",
    "        chosen_token = chosen[\"token_str\"]\n",
    "        sequence = sequence[:first_mask_index] + chosen_token + sequence[first_mask_index+len(\"[MASK]\"):]\n",
    "    return sequence\n",
    "\n",
    "for i in range(NUM_GENERATIONS):\n",
    "    prompt = random.choice(prompts)\n",
    "    # Append MASK tokens\n",
    "    masked_prompt = prompt + \" \" + \" \".join([\"[MASK]\"] * MASK_LENGTH)\n",
    "    generated = iterative_fill_mask(masked_prompt, MASK_LENGTH, fill_mask_pipe)\n",
    "    # Compute zlib ratio\n",
    "    z_ratio = zlib_ratio(generated)\n",
    "    # Compute sliding-window perplexity\n",
    "    ppl_stats = sliding_window_perplexity(generated, model, tokenizer, device, window_size=WINDOW_SIZE)\n",
    "    generations.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_text\": generated,\n",
    "        \"zlib_ratio\": z_ratio,\n",
    "        \"min_window_ppl\": ppl_stats[\"min_ppl\"],\n",
    "        \"avg_window_ppl\": ppl_stats[\"avg_ppl\"]\n",
    "    })\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"[INFO] Generated {i+1} completions.\")\n",
    "\n",
    "with open(GENERATIONS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(generations, f, indent=2)\n",
    "print(f\"[INFO] Saved {len(generations)} completions to {GENERATIONS_FILE}.\")\n",
    "\n",
    "# --- Step C: Membership Inference Filtering ---\n",
    "# Here, we flag suspicious completions as those with high zlib_ratio\n",
    "# and/or very low sliding-window perplexity. For simplicity, we sort by zlib_ratio.\n",
    "generations.sort(key=lambda x: x[\"zlib_ratio\"], reverse=True)\n",
    "top_suspicious = generations[:50]\n",
    "print(\"[INFO] Top 5 suspicious completions by zlib_ratio:\")\n",
    "for j, cand in enumerate(top_suspicious[:5], start=1):\n",
    "    print(f\"{j}. zlib_ratio: {cand['zlib_ratio']:.4f}, min_window_ppl: {cand['min_window_ppl']:.2f}\")\n",
    "    print(f\"Prompt: {cand['prompt']}\")\n",
    "    print(f\"Generated (first 150 chars): {cand['generated_text'][:150]}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# --- Step D: Verification via Embedding-Based Similarity ---\n",
    "# Initialize SentenceTransformer embedding model\n",
    "emb_model = SentenceTransformer(EMB_MODEL_NAME)\n",
    "verified_memorized = []\n",
    "for suspicious in top_suspicious:\n",
    "    gen_text = suspicious[\"generated_text\"]\n",
    "    emb_matches = embedding_similarity(gen_text, corpus, emb_model, field=\"title_abstract\")\n",
    "    if emb_matches:\n",
    "        suspicious[\"embedding_matches\"] = emb_matches\n",
    "        verified_memorized.append(suspicious)\n",
    "\n",
    "print(f\"[INFO] Verified memorized samples via embedding similarity: {len(verified_memorized)}\")\n",
    "for vm in verified_memorized:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"zlib_ratio: {vm['zlib_ratio']:.4f}, min_window_ppl: {vm['min_window_ppl']:.2f}\")\n",
    "    print(f\"Prompt: {vm['prompt']}\")\n",
    "    print(f\"Generated Text: {vm['generated_text']}\")\n",
    "    print(f\"Embedding matches in corpus indices: {vm['embedding_matches']}\")\n",
    "\n",
    "# Save final results\n",
    "results = {\n",
    "    \"generations\": generations,\n",
    "    \"verified_memorized\": verified_memorized\n",
    "}\n",
    "with open(ATTACK_RESULTS, \"w\", encoding=\"utf-8\") as rf:\n",
    "    json.dump(results, rf, indent=2)\n",
    "print(f\"[INFO] Attack results saved to {ATTACK_RESULTS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataCon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97ec761001ea25a843739c998b516e9c906daf472aa6d0129e56e3aea5e0f0e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

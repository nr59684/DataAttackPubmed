{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Refined Black-Box Memorization Attack for BioGPT:\n",
    "- Auto-regressive text generation\n",
    "- Sliding-window perplexity analysis\n",
    "- Embedding-based similarity (BioBERT-NLI) for near-verbatim detection\n",
    "\n",
    "Steps:\n",
    "  1. Loads BioGPT as a black-box model (only using model.generate, plus a minimal forward pass for perplexity).\n",
    "  2. Loads a local PubMed/PMC corpus from papers.json (with 'title' and 'abstract' fields).\n",
    "  3. Builds candidate prompts from titles and partial abstracts.\n",
    "  4. Generates text with top-k/nucleus sampling.\n",
    "  5. Computes:\n",
    "       - zlib ratio (a naive compression-based signal)\n",
    "       - sliding-window perplexity across the generated output\n",
    "  6. Uses domain-specific embeddings to compare each generated completion to the corpus.\n",
    "  7. Prints and saves suspicious results.\n",
    "\n",
    "Requires:\n",
    "  - transformers\n",
    "  - torch\n",
    "  - sentence-transformers\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import zlib\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CONFIGURATION\n",
    "###############################################################################\n",
    "BIOGPT_MODEL_NAME = \"microsoft/BioGPT-Large\"  # Or \"microsoft/BioGPT\"\n",
    "CORPUS_JSON_PATH = \"../../Data/papersNew.json\"             # local PubMed/PMC dataset\n",
    "GENERATIONS_FILE = \"biogpt_generations.json\"\n",
    "ATTACK_RESULTS_FILE = \"biogpt_attack_results.json\"\n",
    "\n",
    "NUM_GENERATIONS = 1000    # number of completions to generate\n",
    "TOKENS_TO_GENERATE = 200  # length of each generated text\n",
    "TEMPERATURE = 0.5\n",
    "TOP_K = 50\n",
    "TOP_P = 0.95\n",
    "\n",
    "WINDOW_SIZE = 50          # sliding window size for perplexity\n",
    "STRIDE_FRACTION = 0.5     # overlap fraction for sliding window (e.g. 0.5 => half overlap)\n",
    "\n",
    "EMB_MODEL_NAME = \"pritamdeka/BioBert-PubMed200kRCT\"  # domain-specific embedding model\n",
    "EMB_SIM_THRESHOLD = 0.85   # similarity threshold to consider near-verbatim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "def load_pubmed_data(json_path: str) -> List[Dict]:\n",
    "    \"\"\"Load local corpus from a JSON file. Each record includes 'title' and 'abstract' sub-dicts with 'full_text' fields.\"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"[ERROR] File not found: {json_path}\")\n",
    "        return []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Lowercase, remove punctuation, and normalize whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def zlib_ratio(txt: str) -> float:\n",
    "    \"\"\"Compute naive compression ratio = len(text) / len(compressed(text)).\"\"\"\n",
    "    if not txt.strip():\n",
    "        return 0.0\n",
    "    compressed = zlib.compress(txt.encode(\"utf-8\"))\n",
    "    return len(txt) / len(compressed)\n",
    "\n",
    "def compute_sliding_window_perplexity(\n",
    "    text: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device: str,\n",
    "    window_size: int = WINDOW_SIZE,\n",
    "    stride_fraction: float = STRIDE_FRACTION\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute perplexity across overlapping windows in 'text'.\n",
    "    Return a dict with min_ppl and avg_ppl.\n",
    "\n",
    "    In a black-box scenario, we assume we can get token-level logprobs from the model.\n",
    "    If that's not possible, you can approximate by multiple generate calls\n",
    "    or skip perplexity entirely.\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = encodings[\"input_ids\"][0].to(device)\n",
    "    seq_len = input_ids.size(0)\n",
    "    if seq_len == 0:\n",
    "        return {\"min_ppl\": None, \"avg_ppl\": None}\n",
    "    window_size = min(window_size, seq_len)\n",
    "    stride = max(1, int(window_size * stride_fraction))\n",
    "\n",
    "    perplexities = []\n",
    "    for start_idx in range(0, seq_len - window_size + 1, stride):\n",
    "        window_ids = input_ids[start_idx : start_idx+window_size].unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(window_ids, labels=window_ids)\n",
    "        loss = outputs.loss\n",
    "        ppl = torch.exp(loss).item()\n",
    "        perplexities.append(ppl)\n",
    "\n",
    "    if not perplexities:\n",
    "        return {\"min_ppl\": None, \"avg_ppl\": None}\n",
    "    return {\n",
    "        \"min_ppl\": min(perplexities),\n",
    "        \"avg_ppl\": sum(perplexities) / len(perplexities)\n",
    "    }\n",
    "\n",
    "def embedding_similarity(\n",
    "    generated_text: str,\n",
    "    corpus: List[Dict],\n",
    "    emb_model,\n",
    "    threshold: float = EMB_SIM_THRESHOLD\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Compare 'generated_text' with each article in 'corpus' via embeddings.\n",
    "    Return indices of articles with cos_sim >= threshold.\n",
    "    \"\"\"\n",
    "    gen_emb = emb_model.encode(generated_text, convert_to_tensor=True)\n",
    "    # For each article, combine title + abstract, then preprocess\n",
    "    corpus_texts = []\n",
    "    for article in corpus:\n",
    "        t = article.get(\"title\", {}).get(\"full_text\", \"\").strip()\n",
    "        a = article.get(\"abstract\", {}).get(\"full_text\", \"\").strip()\n",
    "        combined = preprocess_text(t + \" \" + a)\n",
    "        corpus_texts.append(combined)\n",
    "    corpus_embs = emb_model.encode(corpus_texts, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(gen_emb, corpus_embs)[0]\n",
    "    matches = [i for i, sc in enumerate(scores) if sc.item() >= threshold]\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading BioGPT model: microsoft/BioGPT-Large\n",
      "[INFO] Loading local corpus from ../../Data/papersNew.json\n",
      "[INFO] Found 157833 articles in the local corpus.\n",
      "[INFO] Built 305510 candidate prompts from the corpus.\n"
     ]
    }
   ],
   "source": [
    "# Step A: Load Model & Data\n",
    "print(f\"[INFO] Loading BioGPT model: {BIOGPT_MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BIOGPT_MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(BIOGPT_MODEL_NAME)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"[INFO] Loading local corpus from {CORPUS_JSON_PATH}\")\n",
    "corpus = load_pubmed_data(CORPUS_JSON_PATH)\n",
    "print(f\"[INFO] Found {len(corpus)} articles in the local corpus.\")\n",
    "# Build prompts from title + snippet of abstract\n",
    "prompts = []\n",
    "for article in corpus:\n",
    "    title_text = article.get(\"title\", {}).get(\"full_text\", \"\").strip()\n",
    "    abstract_text = article.get(\"abstract\", {}).get(\"full_text\", \"\").strip()\n",
    "    if title_text:\n",
    "        prompts.append(title_text)\n",
    "    if abstract_text:\n",
    "        words = abstract_text.split()\n",
    "        snippet = \" \".join(words[:20]) if len(words) > 20 else abstract_text\n",
    "        prompts.append(snippet)\n",
    "if not prompts:\n",
    "    prompts = [\"Biomedical research shows\", \"In this study, we explore\"]\n",
    "print(f\"[INFO] Built {len(prompts)} candidate prompts from the corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generated 100 completions.\n",
      "[INFO] Generated 200 completions.\n",
      "[INFO] Generated 300 completions.\n",
      "[INFO] Generated 400 completions.\n",
      "[INFO] Generated 500 completions.\n",
      "[INFO] Generated 600 completions.\n",
      "[INFO] Generated 700 completions.\n",
      "[INFO] Generated 800 completions.\n",
      "[INFO] Generated 900 completions.\n",
      "[INFO] Generated 1000 completions.\n",
      "[INFO] Wrote 1000 completions to biogpt_generations.json.\n"
     ]
    }
   ],
   "source": [
    "# Step B: Generate completions using BioGPT (auto-regressive)\n",
    "completions = []\n",
    "for i in range(NUM_GENERATIONS):\n",
    "    prompt = random.choice(prompts)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=TOKENS_TO_GENERATE,\n",
    "            do_sample=True,\n",
    "            top_k=TOP_K,\n",
    "            top_p=TOP_P,\n",
    "            temperature=TEMPERATURE\n",
    "        )[0]\n",
    "    gen_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    # 1) zlib ratio\n",
    "    z_ratio = zlib_ratio(gen_text)\n",
    "    # 2) sliding-window perplexity\n",
    "    ppl_stats = compute_sliding_window_perplexity(gen_text, model, tokenizer, device, WINDOW_SIZE, STRIDE_FRACTION)\n",
    "    completions.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_text\": gen_text,\n",
    "        \"zlib_ratio\": z_ratio,\n",
    "        \"min_window_ppl\": ppl_stats[\"min_ppl\"],\n",
    "        \"avg_window_ppl\": ppl_stats[\"avg_ppl\"]\n",
    "    })\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"[INFO] Generated {i+1} completions.\")\n",
    "# Save all completions\n",
    "with open(GENERATIONS_FILE, \"w\", encoding=\"utf-8\") as gf:\n",
    "    json.dump(completions, gf, indent=2)\n",
    "print(f\"[INFO] Wrote {len(completions)} completions to {GENERATIONS_FILE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name pritamdeka/BioBert-PubMed200kRCT. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Top 5 suspicious by zlib_ratio:\n",
      "1. zlib_ratio=6.540, min_ppl=4.594261169433594, avg_ppl=8.395771503448486\n",
      "   Prompt: cognitive computing systems are the intelligent systems that thinks, understands and augments the capabilities of human brain by blending the\n",
      "   Generated (first 150 chars): cognitive computing systems are the intelligent systems that thinks, understands and augments the capabilities of human brain by blending the cognitiv...\n",
      "------------------------------------------------------------\n",
      "2. zlib_ratio=6.118, min_ppl=4.158227920532227, avg_ppl=5.9661383628845215\n",
      "   Prompt: blockchain is a disruptive technology for shaping the next era of a healthcare system striving for efficient and effective patient\n",
      "   Generated (first 150 chars): blockchain is a disruptive technology for shaping the next era of a healthcare system striving for efficient and effective patient care. The blockchai...\n",
      "------------------------------------------------------------\n",
      "3. zlib_ratio=5.438, min_ppl=3.486280679702759, avg_ppl=38.868588745594025\n",
      "   Prompt: erratum for toni manzano, cristina fernandez, toni ruiz, and hugo richard, \"ai algorithm qualification,\" accepted article, august 2020.\n",
      "   Generated (first 150 chars): erratum for toni manzano, cristina fernandez, toni ruiz, and hugo richard, \"ai algorithm qualification,\" accepted article, august 2020. < / FREETEXT >...\n",
      "------------------------------------------------------------\n",
      "4. zlib_ratio=4.819, min_ppl=5.206534385681152, avg_ppl=7.745590891156878\n",
      "   Prompt: soft robots have the potential to diminish the need for humans to venture into unsuitable environments or work in extreme\n",
      "   Generated (first 150 chars): soft robots have the potential to diminish the need for humans to venture into unsuitable environments or work in extreme conditions. They are also ca...\n",
      "------------------------------------------------------------\n",
      "5. zlib_ratio=4.742, min_ppl=5.740777969360352, avg_ppl=7.353842599051339\n",
      "   Prompt: to improve the output flow characteristics of the piezoelectric pump in one direction, a new valveless piezoelectric pump with a\n",
      "   Generated (first 150 chars): to improve the output flow characteristics of the piezoelectric pump in one direction, a new valveless piezoelectric pump with a spiral groove was des...\n",
      "------------------------------------------------------------\n",
      "[INFO] Loading domain-specific embedding model: pritamdeka/BioBert-PubMed200kRCT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step C: Filter suspicious (e.g., sort by zlib ratio descending)\n",
    "completions.sort(key=lambda x: x[\"zlib_ratio\"], reverse=True)\n",
    "top_suspicious = completions[:50]\n",
    "print(\"[INFO] Top 5 suspicious by zlib_ratio:\")\n",
    "for j, cand in enumerate(top_suspicious[:5], start=1):\n",
    "    print(f\"{j}. zlib_ratio={cand['zlib_ratio']:.3f}, \"\n",
    "          f\"min_ppl={cand['min_window_ppl']}, \"\n",
    "          f\"avg_ppl={cand['avg_window_ppl']}\")\n",
    "    print(f\"   Prompt: {cand['prompt']}\")\n",
    "    print(f\"   Generated (first 150 chars): {cand['generated_text'][:150]}...\")\n",
    "    print(\"-\" * 60)\n",
    "# Step D: Check embedding-based similarity with domain-specific model\n",
    "print(f\"[INFO] Loading domain-specific embedding model: {EMB_MODEL_NAME}\")\n",
    "emb_model = SentenceTransformer(EMB_MODEL_NAME)\n",
    "verified_memorized = []\n",
    "for suspicious in top_suspicious:\n",
    "    snippet = suspicious[\"generated_text\"]\n",
    "    match_indices = embedding_similarity(snippet, corpus, emb_model, threshold=EMB_SIM_THRESHOLD)\n",
    "    if match_indices:\n",
    "        suspicious[\"embedding_matches\"] = match_indices\n",
    "        verified_memorized.append(suspicious)\n",
    "# Step E: Save and Print final results\n",
    "print(f\"[INFO] Verified memorized completions via embedding similarity: {len(verified_memorized)}\")\n",
    "for item in verified_memorized:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"zlib_ratio={item['zlib_ratio']:.3f}, \"\n",
    "          f\"min_window_ppl={item['min_window_ppl']}, \"\n",
    "          f\"avg_window_ppl={item['avg_window_ppl']}\")\n",
    "    print(f\"Prompt: {item['prompt']}\")\n",
    "    print(f\"Generated Text: {item['generated_text']}\")\n",
    "    print(f\"Matches in corpus indices: {item['embedding_matches']}\")\n",
    "results = {\n",
    "    \"all_completions\": completions,\n",
    "    \"verified_memorized\": verified_memorized\n",
    "}\n",
    "with open(ATTACK_RESULTS_FILE, \"w\", encoding=\"utf-8\") as rf:\n",
    "    json.dump(results, rf, indent=2)\n",
    "print(f\"[INFO] Final results written to {ATTACK_RESULTS_FILE}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataCon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97ec761001ea25a843739c998b516e9c906daf472aa6d0129e56e3aea5e0f0e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

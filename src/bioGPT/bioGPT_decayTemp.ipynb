{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unified Black?Box Memorization Attack Pipeline with Fuzzy n?Gram Matching\n",
    "\n",
    "This script combines:\n",
    "  1. Decaying temperature sampling for text generation from BioGPT.\n",
    "  2. Sliding?window perplexity and zlib compression ratio as membership?inference metrics.\n",
    "  3. Fuzzy n?gram matching (with text preprocessing) to verify near?verbatim overlaps \n",
    "     with a local PubMed abstracts corpus (papers.json).\n",
    "\n",
    "Dependencies:\n",
    "  - transformers\n",
    "  - torch\n",
    "  - sentence-transformers (not used here, since we use fuzzy n-gram matching)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import zlib\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CONFIGURATION\n",
    "###############################################################################\n",
    "BIOGPT_MODEL_NAME = \"microsoft/BioGPT-Large\"  # or \"microsoft/BioGPT\"\n",
    "CORPUS_JSON_PATH = \"../../Data/papersNew.json\"              # local PubMed abstracts data\n",
    "OUTPUT_GENERATIONS = \"biogpt_new_generations.json\"\n",
    "ATTACK_RESULTS_FILE = \"attack_results_unified_fuzzy.json\"\n",
    "\n",
    "NUM_GENERATIONS = 2000      # Number of completions to generate\n",
    "TOKENS_TO_GENERATE = 400    # Number of new tokens per completion\n",
    "INIT_TEMP = 10.0            # Initial temperature for decaying schedule\n",
    "FINAL_TEMP = 1.0            # Final temperature after decay\n",
    "DECAY_TOKENS = 20           # Tokens over which temperature decays\n",
    "TOP_K = 50\n",
    "TOP_P = 0.95\n",
    "\n",
    "# For sliding-window perplexity\n",
    "WINDOW_SIZE = 50\n",
    "STRIDE_FRACTION = 0.5\n",
    "\n",
    "# For fuzzy n-gram matching\n",
    "NGRAM_SIZE = 2              # Use trigrams\n",
    "FUZZY_THRESHOLD = 0.5       # Overlap ratio threshold for a match\n",
    "SUBSTRING_SEARCH_MAX = 2    # Maximum matching articles per candidate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "def load_pubmed_data(json_path: str) -> List[Dict]:\n",
    "    \"\"\"Load local PubMed abstracts from a JSON file.\"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"[ERROR] File not found: {json_path}\")\n",
    "        return []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Lowercase, remove punctuation, and normalize whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def zlib_ratio(txt: str) -> float:\n",
    "    \"\"\"Compute a naive compression ratio as a membership inference metric.\"\"\"\n",
    "    if not txt.strip():\n",
    "        return 0.0\n",
    "    compressed = zlib.compress(txt.encode(\"utf-8\"))\n",
    "    return len(txt) / len(compressed)\n",
    "\n",
    "def compute_sliding_window_perplexity(\n",
    "    text: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device: str,\n",
    "    window_size: int = WINDOW_SIZE,\n",
    "    stride_fraction: float = STRIDE_FRACTION\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute perplexity across overlapping windows in 'text'.\n",
    "    Returns a dict with \"min_ppl\" and \"avg_ppl\".\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = encodings[\"input_ids\"][0].to(device)\n",
    "    seq_len = input_ids.size(0)\n",
    "    if seq_len == 0:\n",
    "        return {\"min_ppl\": None, \"avg_ppl\": None}\n",
    "    win_size = min(window_size, seq_len)\n",
    "    stride = max(1, int(win_size * stride_fraction))\n",
    "    perplexities = []\n",
    "    for start_idx in range(0, seq_len - win_size + 1, stride):\n",
    "        window_ids = input_ids[start_idx : start_idx + win_size].unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(window_ids, labels=window_ids)\n",
    "        loss = outputs.loss\n",
    "        ppl = torch.exp(loss).item()\n",
    "        perplexities.append(ppl)\n",
    "    if not perplexities:\n",
    "        return {\"min_ppl\": None, \"avg_ppl\": None}\n",
    "    return {\"min_ppl\": min(perplexities), \"avg_ppl\": sum(perplexities) / len(perplexities)}\n",
    "\n",
    "def fuzzy_ngram_search(\n",
    "    snippet: str,\n",
    "    corpus: List[Dict],\n",
    "    n: int = NGRAM_SIZE,\n",
    "    threshold: float = FUZZY_THRESHOLD,\n",
    "    max_results: int = SUBSTRING_SEARCH_MAX\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Compute the n-gram Jaccard similarity between the preprocessed 'snippet' and each article's\n",
    "    combined title and abstract in 'corpus'. Returns indices of articles where the similarity\n",
    "    is at least 'threshold'.\n",
    "    \"\"\"\n",
    "    snippet_processed = preprocess_text(snippet)\n",
    "    snippet_tokens = snippet_processed.split()\n",
    "    if len(snippet_tokens) < n:\n",
    "        snippet_ngrams = {tuple(snippet_tokens)}\n",
    "    else:\n",
    "        snippet_ngrams = set(zip(*[snippet_tokens[i:] for i in range(n)]))\n",
    "    \n",
    "    matches = []\n",
    "    for i, article in enumerate(corpus):\n",
    "        title = \"\"\n",
    "        abstract = \"\"\n",
    "        title_info = article.get(\"title\", {})\n",
    "        if isinstance(title_info, dict):\n",
    "            title = title_info.get(\"full_text\", \"\") or \"\"\n",
    "        else:\n",
    "            title = title_info or \"\"\n",
    "        abstract_info = article.get(\"abstract\", {})\n",
    "        if isinstance(abstract_info, dict):\n",
    "            abstract = abstract_info.get(\"full_text\", \"\") or \"\"\n",
    "        else:\n",
    "            abstract = abstract_info or \"\"\n",
    "        combined = preprocess_text(title + \" \" + abstract)\n",
    "        combined_tokens = combined.split()\n",
    "        if len(combined_tokens) < n:\n",
    "            combined_ngrams = {tuple(combined_tokens)}\n",
    "        else:\n",
    "            combined_ngrams = set(zip(*[combined_tokens[i:] for i in range(n)]))\n",
    "        \n",
    "        if not snippet_ngrams or not combined_ngrams:\n",
    "            continue\n",
    "        \n",
    "        intersection = snippet_ngrams.intersection(combined_ngrams)\n",
    "        union = snippet_ngrams.union(combined_ngrams)\n",
    "        similarity = len(intersection) / len(union) if union else 0.0\n",
    "        \n",
    "        if similarity >= threshold:\n",
    "            matches.append(i)\n",
    "            if len(matches) >= max_results:\n",
    "                break\n",
    "    return matches\n",
    "\n",
    "def generate_with_decaying_temperature(\n",
    "    model, tokenizer, prompt, max_new_tokens=200,\n",
    "    init_temp=10.0, final_temp=1.0, decay_tokens=20,\n",
    "    top_k=50, top_p=0.95\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text from the model using a decaying temperature schedule.\n",
    "    Temperature decays linearly from init_temp to final_temp over the first decay_tokens,\n",
    "    then remains at final_temp.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = input_ids.clone()\n",
    "    \n",
    "    for i in range(max_new_tokens):\n",
    "        if i < decay_tokens:\n",
    "            temperature = init_temp - (init_temp - final_temp) * (i / decay_tokens)\n",
    "        else:\n",
    "            temperature = final_temp\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            generated_ids,\n",
    "            max_new_tokens=1,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        generated_ids = torch.cat([generated_ids, outputs[:, -1:]], dim=-1)\n",
    "    \n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading BioGPT model: microsoft/BioGPT-Large\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(BIOGPT_MODEL_NAME)\n\u001b[1;32m      5\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[INFO] Loading corpus from \u001b[39m\u001b[39m{\u001b[39;00mCORPUS_JSON_PATH\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m corpus \u001b[39m=\u001b[39m load_pubmed_data(CORPUS_JSON_PATH)\n",
      "File \u001b[0;32m~/anaconda3/envs/dataCon/lib/python3.11/site-packages/transformers/modeling_utils.py:3162\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[39mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3158\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3159\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3160\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3161\u001b[0m         )\n\u001b[0;32m-> 3162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dataCon/lib/python3.11/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/dataCon/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataCon/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataCon/lib/python3.11/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/dataCon/lib/python3.11/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Step A: Load BioGPT Model & Corpus\n",
    "print(f\"[INFO] Loading BioGPT model: {BIOGPT_MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BIOGPT_MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(BIOGPT_MODEL_NAME)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(f\"[INFO] Loading corpus from {CORPUS_JSON_PATH}\")\n",
    "corpus = load_pubmed_data(CORPUS_JSON_PATH)\n",
    "print(f\"[INFO] Loaded {len(corpus)} articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Built 305510 candidate prompts.\n"
     ]
    }
   ],
   "source": [
    "# Step B: Build Candidate Prompts (from title and abstract snippet)\n",
    "prompts = []\n",
    "for article in corpus:\n",
    "    title_info = article.get(\"title\", {})\n",
    "    if isinstance(title_info, dict):\n",
    "        title_text = title_info.get(\"full_text\", \"\").strip()\n",
    "    else:\n",
    "        title_text = (title_info or \"\").strip()\n",
    "    abstract_info = article.get(\"abstract\", {})\n",
    "    if isinstance(abstract_info, dict):\n",
    "        abstract_text = abstract_info.get(\"full_text\", \"\").strip()\n",
    "    else:\n",
    "        abstract_text = (abstract_info or \"\").strip()\n",
    "    if title_text:\n",
    "        prompts.append(title_text)\n",
    "    if abstract_text:\n",
    "        words = abstract_text.split()\n",
    "        snippet = \" \".join(words[:20]) if len(words) > 20 else abstract_text\n",
    "        prompts.append(snippet)\n",
    "if not prompts:\n",
    "    prompts = [\"Biomedical research shows\", \"In this study, we explore\"]\n",
    "print(f\"[INFO] Built {len(prompts)} candidate prompts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generated 100 completions.\n",
      "[INFO] Generated 200 completions.\n",
      "[INFO] Generated 300 completions.\n",
      "[INFO] Generated 400 completions.\n",
      "[INFO] Generated 500 completions.\n",
      "[INFO] Generated 600 completions.\n",
      "[INFO] Generated 700 completions.\n",
      "[INFO] Generated 800 completions.\n",
      "[INFO] Generated 900 completions.\n",
      "[INFO] Generated 1000 completions.\n",
      "[INFO] Generated 1100 completions.\n",
      "[INFO] Generated 1200 completions.\n",
      "[INFO] Generated 1300 completions.\n",
      "[INFO] Generated 1400 completions.\n",
      "[INFO] Generated 1500 completions.\n",
      "[INFO] Generated 1600 completions.\n",
      "[INFO] Generated 1700 completions.\n",
      "[INFO] Generated 1800 completions.\n",
      "[INFO] Generated 1900 completions.\n",
      "[INFO] Generated 2000 completions.\n",
      "[INFO] Saved 2000 completions to biogpt_new_generations.json.\n"
     ]
    }
   ],
   "source": [
    "# Step C: Generate Completions using Decaying Temperature Sampling\n",
    "completions = []\n",
    "for i in range(NUM_GENERATIONS):\n",
    "    prompt = random.choice(prompts)\n",
    "    gen_text = generate_with_decaying_temperature(\n",
    "        model, tokenizer, prompt,\n",
    "        max_new_tokens=TOKENS_TO_GENERATE,\n",
    "        init_temp=INIT_TEMP,\n",
    "        final_temp=FINAL_TEMP,\n",
    "        decay_tokens=DECAY_TOKENS,\n",
    "        top_k=TOP_K,\n",
    "        top_p=TOP_P\n",
    "    )\n",
    "    ppl_stats = compute_sliding_window_perplexity(gen_text, model, tokenizer, device)\n",
    "    z_ratio = zlib_ratio(gen_text)\n",
    "    completions.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_text\": gen_text,\n",
    "        \"zlib_ratio\": z_ratio,\n",
    "        \"min_window_ppl\": ppl_stats[\"min_ppl\"],\n",
    "        \"avg_window_ppl\": ppl_stats[\"avg_ppl\"]\n",
    "    })\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"[INFO] Generated {i+1} completions.\")\n",
    "\n",
    "with open(OUTPUT_GENERATIONS, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(completions, f, indent=2)\n",
    "print(f\"[INFO] Saved {len(completions)} completions to {OUTPUT_GENERATIONS}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Top 5 suspicious completions by zlib_ratio:\n",
      "1. zlib_ratio: 4.215, min_window_ppl: 4.580333709716797, avg_window_ppl: 18.493740367889405\n",
      "   Prompt: perceived barriers to and facilitators of being physically active during adjuvant cancer treatment.\n",
      "   Generated (first 150 chars): perceived barriers to and facilitators of being physically active during adjuvant cancer treatment. < & class member 3-A mixed methodology for qualita...\n",
      "------------------------------------------------------------\n",
      "2. zlib_ratio: 3.490, min_window_ppl: 2.3643605709075928, avg_window_ppl: 12.249446058273316\n",
      "   Prompt: this article will give a brief history, review the latest guidelines, discuss risk factors and sources, and discuss screening, diagnosis,\n",
      "   Generated (first 150 chars): this article will give a brief history, review the latest guidelines, discuss risk factors and sources, and discuss screening, diagnosis, therapy appr...\n",
      "------------------------------------------------------------\n",
      "3. zlib_ratio: 3.478, min_window_ppl: 2.248488664627075, avg_window_ppl: 12.20253423055013\n",
      "   Prompt: reliable interconnection is a primary requirement for the fabrication of electronic or electromechanical devices in the bottom-up nanotechnology. at the\n",
      "   Generated (first 150 chars): reliable interconnection is a primary requirement for the fabrication of electronic or electromechanical devices in the bottom-up nanotechnology. at t...\n",
      "------------------------------------------------------------\n",
      "4. zlib_ratio: 3.393, min_window_ppl: 2.411064863204956, avg_window_ppl: 12.375022919972738\n",
      "   Prompt: the interrelation of interoception, cognitive appraisal of bodily signals and conscious self-regulatory behavior is insufficiently understood although it may be\n",
      "   Generated (first 150 chars): the interrelation of interoception, cognitive appraisal of bodily signals and conscious self-regulatory behavior is insufficiently understood although...\n",
      "------------------------------------------------------------\n",
      "5. zlib_ratio: 3.370, min_window_ppl: 2.1371915340423584, avg_window_ppl: 12.480562909444172\n",
      "   Prompt: empathic responses are reduced to competitive but not non-competitive outgroups.\n",
      "   Generated (first 150 chars): empathic responses are reduced to competitive but not non-competitive outgroups. An interaction occurs suggesting stronger inward-- (social inclusion ...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step D: Membership Inference Filtering\n",
    "# We sort by zlib_ratio (descending) to flag highly repetitive (and thus potentially memorized) outputs.\n",
    "completions.sort(key=lambda x: x[\"zlib_ratio\"], reverse=True)\n",
    "top_suspicious = completions[:50]\n",
    "print(\"[INFO] Top 5 suspicious completions by zlib_ratio:\")\n",
    "for j, cand in enumerate(top_suspicious[:5], start=1):\n",
    "    print(f\"{j}. zlib_ratio: {cand['zlib_ratio']:.3f}, min_window_ppl: {cand['min_window_ppl']}, avg_window_ppl: {cand['avg_window_ppl']}\")\n",
    "    print(f\"   Prompt: {cand['prompt']}\")\n",
    "    print(f\"   Generated (first 150 chars): {cand['generated_text'][:150]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Verified memorized completions via fuzzy n-gram matching: 0\n"
     ]
    }
   ],
   "source": [
    "# Step E: Verification using Fuzzy n-Gram Matching\n",
    "verified_memorized = []\n",
    "for suspicious in top_suspicious:\n",
    "    snippet = suspicious[\"generated_text\"]\n",
    "    matches = fuzzy_ngram_search(snippet, corpus, n=NGRAM_SIZE, threshold=FUZZY_THRESHOLD, max_results=SUBSTRING_SEARCH_MAX)\n",
    "    if matches:\n",
    "        suspicious[\"fuzzy_matches\"] = matches\n",
    "        verified_memorized.append(suspicious)\n",
    "\n",
    "print(f\"[INFO] Verified memorized completions via fuzzy n-gram matching: {len(verified_memorized)}\")\n",
    "for item in verified_memorized:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"zlib_ratio: {item['zlib_ratio']:.3f}, min_window_ppl: {item['min_window_ppl']}, avg_window_ppl: {item['avg_window_ppl']}\")\n",
    "    print(f\"Prompt: {item['prompt']}\")\n",
    "    print(f\"Generated Text: {item['generated_text']}\")\n",
    "    print(f\"Fuzzy Matches (corpus indices): {item['fuzzy_matches']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Final attack results saved to attack_results_unified_fuzzy.json\n"
     ]
    }
   ],
   "source": [
    "# Step F: Save Final Attack Results\n",
    "results = {\n",
    "    \"all_completions\": completions,\n",
    "    \"verified_memorized\": verified_memorized\n",
    "}\n",
    "with open(ATTACK_RESULTS_FILE, \"w\", encoding=\"utf-8\") as rf:\n",
    "    json.dump(results, rf, indent=2)\n",
    "print(f\"[INFO] Final attack results saved to {ATTACK_RESULTS_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataCon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97ec761001ea25a843739c998b516e9c906daf472aa6d0129e56e3aea5e0f0e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

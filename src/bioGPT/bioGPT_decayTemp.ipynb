{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Black?Box Memorization Attack Using BioGPT on PubMed Abstracts\n",
    "with Decaying Temperature Sampling\n",
    "\n",
    "This script:\n",
    "  1. Loads BioGPT (auto?regressive) as a black?box model.\n",
    "  2. Loads local PubMed abstracts data from a JSON file (papers.json).\n",
    "  3. Builds candidate prompts from titles and abstract snippets.\n",
    "  4. Generates completions using decaying temperature sampling.\n",
    "  5. Computes membership-inference metrics (naive zlib ratio and perplexity).\n",
    "  6. Applies fuzzy matching (naive substring or n?gram matching) to verify if a generated\n",
    "     snippet is present (or near?verbatim) in the local corpus.\n",
    "  7. Outputs and saves the results.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import zlib\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Configuration\n",
    "###############################################################################\n",
    "MODEL_NAME = \"microsoft/BioGPT-Large\"  # or \"microsoft/BioGPT\"\n",
    "PMC_JSON_PATH = \"../Data/pubmed_2010_2024_intelligence.json\"    # local PMC data JSON file\n",
    "OUTPUT_GENERATIONS = \"biogpt_generations.json\"\n",
    "ATTACK_RESULTS = \"attack_results.json\"\n",
    "\n",
    "NUM_GENERATIONS = 2000     # Total completions to generate\n",
    "TOKENS_TO_GENERATE = 400   # Number of new tokens per completion\n",
    "INIT_TEMP = 10.0           # Initial temperature for decaying schedule\n",
    "FINAL_TEMP = 1.0           # Final temperature after decay\n",
    "DECAY_TOKENS = 20          # Number of tokens over which temperature decays\n",
    "TOP_K = 50\n",
    "TOP_P = 0.95\n",
    "TEMPERATURE = None         # Not used directly; replaced by decaying schedule\n",
    "SUBSTRING_SEARCH_MAX = 2   # Maximum matches per candidate snippet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Helper Functions\n",
    "###############################################################################\n",
    "def load_pmc_data(json_path: str) -> List[Dict]:\n",
    "    \"\"\"Load local PMC data from JSON file.\"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"[ERROR] File not found: {json_path}\")\n",
    "        return []\n",
    "    data=[]\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # skip empty lines\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def zlib_ratio(txt: str) -> float:\n",
    "    \"\"\"Compute a naive zlib compression ratio as a membership inference metric.\"\"\"\n",
    "    if not txt.strip():\n",
    "        return 0.0\n",
    "    compressed = zlib.compress(txt.encode(\"utf-8\"))\n",
    "    return len(txt) / len(compressed)\n",
    "\n",
    "def fuzzy_ngram_search(snippet: str, corpus: List[Dict], n: int = 3, threshold: float = 0.5, max_results: int = 2) -> List[int]:\n",
    "    \"\"\"\n",
    "    Compute n-gram overlap similarity between the snippet and the combined text\n",
    "    (title + abstract) from each article in the corpus.\n",
    "    \n",
    "    Returns indices of articles where the overlap ratio (intersection/union) \n",
    "    is at least the threshold. Adjust n and threshold as needed.\n",
    "    \"\"\"\n",
    "    snippet = snippet.lower()\n",
    "    snippet_tokens = snippet.split()\n",
    "    if len(snippet_tokens) < n:\n",
    "        snippet_ngrams = set([tuple(snippet_tokens)])\n",
    "    else:\n",
    "        snippet_ngrams = set(zip(*[snippet_tokens[i:] for i in range(n)]))\n",
    "    \n",
    "    matches = []\n",
    "    for i, article in enumerate(corpus):\n",
    "        title = article.get(\"title\", {}) or \"\"\n",
    "        abstract = article.get(\"abstract\", {}) or \"\"\n",
    "        combined = (title + \" \" + abstract).lower()\n",
    "        combined_tokens = combined.split()\n",
    "        if len(combined_tokens) < n:\n",
    "            combined_ngrams = set([tuple(combined_tokens)])\n",
    "        else:\n",
    "            combined_ngrams = set(zip(*[combined_tokens[i:] for i in range(n)]))\n",
    "        \n",
    "        if not snippet_ngrams or not combined_ngrams:\n",
    "            continue\n",
    "\n",
    "        intersection = snippet_ngrams.intersection(combined_ngrams)\n",
    "        union = snippet_ngrams.union(combined_ngrams)\n",
    "        similarity = len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "        if similarity >= threshold:\n",
    "            matches.append(i)\n",
    "            if len(matches) >= max_results:\n",
    "                break\n",
    "\n",
    "    return matches\n",
    "\n",
    "\n",
    "\n",
    "def compute_perplexity(text: str, model, tokenizer, device: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute perplexity for a given text using the model.\n",
    "    This function uses the model in a black-box way: we simply pass the text and get loss.\n",
    "    Note: In a real black-box API you might not have this ability.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    # Use labels identical to inputs for computing loss\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss  # average negative log likelihood per token\n",
    "    perplexity = torch.exp(loss)\n",
    "    return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_decaying_temperature(model, tokenizer, prompt, max_new_tokens=400,\n",
    "                                       init_temp=10.0, final_temp=1.0, decay_tokens=20,\n",
    "                                       top_k=50, top_p=0.95):\n",
    "    \"\"\"\n",
    "    Generate text from the model with a temperature that decays over the first `decay_tokens`.\n",
    "    \n",
    "    Args:\n",
    "      model: The auto-regressive model (BioGPT).\n",
    "      tokenizer: The corresponding tokenizer.\n",
    "      prompt: The input prompt (string).\n",
    "      max_new_tokens: Total number of new tokens to generate.\n",
    "      init_temp: Initial temperature.\n",
    "      final_temp: Temperature after decay.\n",
    "      decay_tokens: Number of tokens over which temperature decays.\n",
    "      top_k, top_p: Sampling parameters.\n",
    "      \n",
    "    Returns:\n",
    "      The generated text (prompt + completion).\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = input_ids.clone()\n",
    "\n",
    "    for i in range(max_new_tokens):\n",
    "        if i < decay_tokens:\n",
    "            temperature = init_temp - (init_temp - final_temp) * (i / decay_tokens)\n",
    "        else:\n",
    "            temperature = final_temp\n",
    "\n",
    "        outputs = model.generate(\n",
    "            generated_ids,\n",
    "            max_new_tokens=1,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        generated_ids = torch.cat([generated_ids, outputs[:, -1:]], dim=-1)\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading BioGPT model: microsoft/BioGPT\n",
      "[INFO] Loading local PMC data from ../Data/pubmed_2010_2024_intelligence.json\n",
      "[INFO] Loaded 54583 articles from local corpus.\n"
     ]
    }
   ],
   "source": [
    "# --- Step A: Load Model & Local Data ---\n",
    "print(f\"[INFO] Loading BioGPT model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(f\"[INFO] Loading local PMC data from {PMC_JSON_PATH}\")\n",
    "corpus = load_pmc_data(PMC_JSON_PATH)\n",
    "print(f\"[INFO] Loaded {len(corpus)} articles from local corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using 106151 candidate prompts.\n"
     ]
    }
   ],
   "source": [
    "# --- Step B: Build Candidate Prompts ---\n",
    "prompts = []\n",
    "for article in corpus:\n",
    "    title = article.get(\"title\", \"\").strip()\n",
    "    if title:\n",
    "        prompts.append(title)\n",
    "    abstract = article.get(\"abstract\", \"\").strip()\n",
    "    if abstract:\n",
    "        words = abstract.split()\n",
    "        prompt_abstract = \" \".join(words[:20]) if len(words) > 20 else abstract\n",
    "        prompts.append(prompt_abstract)\n",
    "if not prompts:\n",
    "    prompts = [\"Biomedical research shows\", \"In this study, we explore\"]\n",
    "print(f\"[INFO] Using {len(prompts)} candidate prompts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generated 100 completions.\n",
      "[INFO] Generated 200 completions.\n",
      "[INFO] Generated 300 completions.\n",
      "[INFO] Generated 400 completions.\n",
      "[INFO] Generated 500 completions.\n",
      "[INFO] Generated 600 completions.\n",
      "[INFO] Generated 700 completions.\n",
      "[INFO] Generated 800 completions.\n",
      "[INFO] Generated 900 completions.\n",
      "[INFO] Generated 1000 completions.\n",
      "[INFO] Generated 1100 completions.\n",
      "[INFO] Generated 1200 completions.\n",
      "[INFO] Generated 1300 completions.\n",
      "[INFO] Generated 1400 completions.\n",
      "[INFO] Generated 1500 completions.\n",
      "[INFO] Generated 1600 completions.\n",
      "[INFO] Generated 1700 completions.\n",
      "[INFO] Generated 1800 completions.\n",
      "[INFO] Generated 1900 completions.\n",
      "[INFO] Generated 2000 completions.\n",
      "[INFO] Saved 2000 completions to biogpt_generations.json.\n"
     ]
    }
   ],
   "source": [
    "# --- Step C: Generate Text Completions (Black-Box) ---\n",
    "generations = []\n",
    "for i in range(NUM_GENERATIONS):\n",
    "    prompt = random.choice(prompts)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=TOKENS_TO_GENERATE,\n",
    "            do_sample=True,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_k=TOP_K,\n",
    "            top_p=TOP_P\n",
    "        )[0]\n",
    "    gen_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    ppl = compute_perplexity(gen_text, model, tokenizer, device)\n",
    "    generations.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_text\": gen_text,\n",
    "        \"perplexity\": ppl,\n",
    "        \"zlib_ratio\": zlib_ratio(gen_text)\n",
    "    })\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"[INFO] Generated {i+1} completions.\")\n",
    "\n",
    "with open(OUTPUT_GENERATIONS, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(generations, f, indent=2)\n",
    "print(f\"[INFO] Saved {len(generations)} completions to {OUTPUT_GENERATIONS}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Top 5 suspicious completions by zlib_ratio:\n",
      "1. zlib_ratio: 1.9537, perplexity: 8.04\n",
      "Prompt: In this paper, we hypothesize that education is associated with a higher efficiency of health investment, yet that this efficiency\n",
      "Generated (first 100 chars): In this paper, we hypothesize that education is associated with a higher efficiency of health invest...\n",
      "------------------------------------------------------------\n",
      "2. zlib_ratio: 1.9352, perplexity: 11.60\n",
      "Prompt: Effective implementation of artificial intelligence in behavioral healthcare delivery depends on overcoming challenges that are pronounced in this domain. Self\n",
      "Generated (first 100 chars): Effective implementation of artificial intelligence in behavioral healthcare delivery depends on ove...\n",
      "------------------------------------------------------------\n",
      "3. zlib_ratio: 1.8832, perplexity: 9.35\n",
      "Prompt: Relatively lower executive functioning is characteristic of individuals with schizophrenia. As low socio-economic status (SES) early in life (i.e. parent\n",
      "Generated (first 100 chars): Relatively lower executive functioning is characteristic of individuals with schizophrenia. As low s...\n",
      "------------------------------------------------------------\n",
      "4. zlib_ratio: 1.8831, perplexity: 12.42\n",
      "Prompt: To co-design a patient and family-initiated intervention to improve the detection and escalation of patient deterioration on acute adult hospital\n",
      "Generated (first 100 chars): To co-design a patient and family-initiated intervention to improve the detection and escalation of ...\n",
      "------------------------------------------------------------\n",
      "5. zlib_ratio: 1.8600, perplexity: 9.44\n",
      "Prompt: Research on relationships between personality and cognitive abilities has so far resulted in inconsistent findings regarding the strength of the\n",
      "Generated (first 100 chars): Research on relationships between personality and cognitive abilities has so far resulted in inconsi...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Step D: Membership Inference Filtering ---\n",
    "# Here we combine two metrics: perplexity and zlib_ratio.\n",
    "# In this example, we simply rank by zlib_ratio (higher means more repeated structure)\n",
    "generations.sort(key=lambda x: x[\"zlib_ratio\"], reverse=True)\n",
    "top_suspicious = generations[:50]  # Top 50 candidates by zlib_ratio\n",
    "print(\"[INFO] Top 5 suspicious completions by zlib_ratio:\")\n",
    "for j, cand in enumerate(top_suspicious[:5], start=1):\n",
    "    print(f\"{j}. zlib_ratio: {cand['zlib_ratio']:.4f}, perplexity: {cand['perplexity']:.2f}\")\n",
    "    print(f\"Prompt: {cand['prompt']}\")\n",
    "    print(f\"Generated (first 100 chars): {cand['generated_text'][:100]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Effective implementation of artificial intelligence in behavioral healthcare delivery depends on overcoming challenges that are pronounced in this domain. Self',\n",
       " 'generated_text': 'Effective implementation of artificial intelligence in behavioral healthcare delivery depends on overcoming challenges that are pronounced in this domain. Self-directed learning is essential in the implementation of AI / EAI for behavior healthcare delivery; the success of AI / EAI for behavior healthcare depends not only on the development of AI / EAI applications but also on how the behavior healthcare professionals themselves, and the patients who use it, are able to effectively learn to use the AI / EAI algorithms, with appropriate support from their peers.',\n",
       " 'perplexity': 11.59685230255127,\n",
       " 'zlib_ratio': 1.9351535836177474}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_suspicious[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Verified memorized samples (exact substring matches): 0\n"
     ]
    }
   ],
   "source": [
    "# --- Step E: Verification via Substring Search ---\n",
    "verified_memorized = []\n",
    "for suspicious in top_suspicious:\n",
    "    snippet = suspicious[\"generated_text\"]\n",
    "    # Use fuzzy matching (with trigrams and threshold of 0.5, adjust as needed)\n",
    "    matches = fuzzy_ngram_search(snippet, corpus, n=3, threshold=0.5, max_results=SUBSTRING_SEARCH_MAX)\n",
    "    if matches:\n",
    "        suspicious[\"matches\"] = matches\n",
    "        verified_memorized.append(suspicious)\n",
    "\n",
    "print(f\"[INFO] Verified memorized samples (exact substring matches): {len(verified_memorized)}\")\n",
    "for v in verified_memorized:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"zlib_ratio: {v['zlib_ratio']:.4f}, perplexity: {v['perplexity']:.2f}\")\n",
    "    print(f\"Prompt: {v['prompt']}\")\n",
    "    print(f\"Generated Text: {v['generated_text']}\")\n",
    "    print(f\"Found in corpus indices: {v['matches']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Attack results saved to attack_results.json\n"
     ]
    }
   ],
   "source": [
    "# --- Save Final Attack Results ---\n",
    "results = {\n",
    "    \"generations\": generations,\n",
    "    \"verified_memorized\": verified_memorized\n",
    "}\n",
    "with open(ATTACK_RESULTS, \"w\", encoding=\"utf-8\") as rf:\n",
    "    json.dump(results, rf, indent=2)\n",
    "print(f\"[INFO] Attack results saved to {ATTACK_RESULTS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataCon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97ec761001ea25a843739c998b516e9c906daf472aa6d0129e56e3aea5e0f0e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "import json\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your email address for the Entrez API\n",
    "Entrez.email = \"<email_address_removed>\" #use your registered email address\n",
    "\n",
    "def fetch_details(id_list):\n",
    "    \"\"\"\n",
    "    Fetch details for a list of PubMed article IDs.\n",
    "\n",
    "    Parameters:\n",
    "        id_list (list): List of PubMed article IDs.\n",
    "\n",
    "    Returns:\n",
    "        dict: Details of the fetched articles.\n",
    "    \"\"\"\n",
    "    ids = ','.join(id_list)\n",
    "    handle = Entrez.efetch(db='pmc', id=ids, retmode='xml')\n",
    "    papers = Entrez.read(handle)\n",
    "    return papers\n",
    "\n",
    "def generate_date_intervals(start_date, end_date, days):\n",
    "    \"\"\"\n",
    "    Generate date intervals between a start and end date.\n",
    "\n",
    "    Parameters:\n",
    "        start_date (str): Start date in the format 'YYYY/MM/DD'.\n",
    "        end_date (str): End date in the format 'YYYY/MM/DD'.\n",
    "        days (int): Number of days for each interval.\n",
    "\n",
    "    Returns:\n",
    "        list: List of date intervals.\n",
    "    \"\"\"\n",
    "    date_format = \"%Y/%m/%d\"\n",
    "    start_datetime = datetime.strptime(start_date, date_format)\n",
    "    end_datetime = datetime.strptime(end_date, date_format)\n",
    "    current_datetime = start_datetime\n",
    "    date_intervals = []\n",
    "\n",
    "    while current_datetime < end_datetime:\n",
    "        date_intervals.append(current_datetime.strftime(date_format))\n",
    "        current_datetime += timedelta(days=days)\n",
    "\n",
    "    date_intervals.append(end_datetime.strftime(date_format))\n",
    "    return date_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_body_text(body_section):\n",
    "    \"\"\"\n",
    "    Recursively extract text from 'body' or nested 'sec' elements in the PMC XML structure \n",
    "    returned by Entrez.\n",
    "    \"\"\"\n",
    "    # This is a simplistic example: the structure can be deeply nested\n",
    "    # with body['sec'] -> list of sections, each with paragraphs, etc.\n",
    "    # Adjust to your doc structure\n",
    "\n",
    "    text_content = []\n",
    "\n",
    "    if isinstance(body_section, dict):\n",
    "        # Possibly has 'p', 'sec' keys\n",
    "        if 'p' in body_section:\n",
    "            # 'p' could be a list of paragraphs\n",
    "            paragraphs = body_section['p']\n",
    "            if isinstance(paragraphs, list):\n",
    "                for p in paragraphs:\n",
    "                    # p might be string or dict if there's sub-structure\n",
    "                    if isinstance(p, str):\n",
    "                        text_content.append(p)\n",
    "                    elif isinstance(p, dict):\n",
    "                        text_content.append(str(p))\n",
    "            else:\n",
    "                # single paragraph\n",
    "                text_content.append(str(paragraphs))\n",
    "\n",
    "        if 'sec' in body_section:\n",
    "            # 'sec' might be a list of sub-sections\n",
    "            subsections = body_section['sec']\n",
    "            if isinstance(subsections, list):\n",
    "                for sec_el in subsections:\n",
    "                    text_content.append(extract_body_text(sec_el))\n",
    "            else:\n",
    "                text_content.append(extract_body_text(subsections))\n",
    "    elif isinstance(body_section, list):\n",
    "        # body_section might be a list of sections\n",
    "        for item in body_section:\n",
    "            text_content.append(extract_body_text(item))\n",
    "    elif isinstance(body_section, str):\n",
    "        # direct string\n",
    "        text_content.append(body_section)\n",
    "\n",
    "    # Join all extracted text with spaces\n",
    "    return \" \".join([txt.strip() for txt in text_content if txt.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your search query parameters\n",
    "search_query = \"intelligence\"\n",
    "start_date = \"2014/01/01\"\n",
    "end_date = \"2024/12/31\"\n",
    "days_interval = 200\n",
    "\n",
    "# Generate date intervals\n",
    "intervals = generate_date_intervals(start_date, end_date, days_interval)\n",
    "\n",
    "# Initialize lists to store data\n",
    "data_list = []\n",
    "\n",
    "chunk_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through date intervals\n",
    "for interval_start, interval_end in zip(intervals[:-1], intervals[1:]):\n",
    "    # Use esearch to get the list of IDs matching your criteria for the current interval\n",
    "    search_handle = Entrez.esearch(db='pmc', term=search_query, mindate=interval_start, maxdate=interval_end, retmax=20000)\n",
    "    search_results = Entrez.read(search_handle)\n",
    "    search_handle.close()\n",
    "\n",
    "    # Extract the list of IDs\n",
    "    studies_id_list = search_results['IdList']\n",
    "\n",
    "    # Fetch details in chunks\n",
    "    for chunk_i in range(0, len(studies_id_list), chunk_size):\n",
    "        chunk = studies_id_list[chunk_i:chunk_i + chunk_size]\n",
    "        papers = fetch_details(chunk)\n",
    "        for record in papers:\n",
    "            try:\n",
    "                metadata = record['OAI-PMH']['GetRecord']['record']['metadata']\n",
    "                article = metadata['article']  # root of the article\n",
    "                front = article['front']  # front matter\n",
    "                body = article.get('body', {})  # main text sections\n",
    "            except KeyError:\n",
    "                continue\n",
    "            data = {\n",
    "                \"title\": {\n",
    "                    \"full_text\": \"\",\n",
    "                    \"tokens\": []\n",
    "                },\n",
    "                \"authors\": [],\n",
    "                \"affiliations\": [],\n",
    "                \"identifiers\": {},\n",
    "                \"journal\": \"\",\n",
    "                \"language\": \"\",\n",
    "                \"abstract\": {\n",
    "                    \"full_text\": \"\",\n",
    "                    \"tokens\": []\n",
    "                },\n",
    "                \"year\": \"\",\n",
    "                \"month\": \"\",\n",
    "                \"keywords\": [],\n",
    "                \"full_text\": \"\",\n",
    "                \"references\": []\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                article_meta = front['article-meta']\n",
    "                data[\"title\"][\"full_text\"] = article_meta['title-group']['article-title']\n",
    "                data[\"title\"][\"tokens\"] = data[\"title\"][\"full_text\"].split()\n",
    "                for author in paper['MedlineCitation']['Article']['AuthorList']:\n",
    "                    author_name = f\"{author.get('LastName', '')}, {author.get('ForeName', '')}\"\n",
    "                    data[\"authors\"].append(author_name)\n",
    "            except:\n",
    "                ...\n",
    "\n",
    "            try:\n",
    "                data[\"abstract\"][\"full_text\"] = paper['MedlineCitation']['Article']['Abstract']['AbstractText'][0].lower()\n",
    "                data[\"abstract\"][\"tokens\"] = data[\"abstract\"][\"full_text\"].split()\n",
    "            except:\n",
    "                data[\"abstract\"][\"full_text\"] = ''\n",
    "\n",
    "            data[\"journal\"] = paper['MedlineCitation']['Article']['Journal']['Title'].lower()\n",
    "            data[\"language\"] = paper['MedlineCitation']['Article']['Language'][0].lower()\n",
    "\n",
    "            try:\n",
    "                data[\"year\"] = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year'].lower()\n",
    "            except:\n",
    "                data[\"year\"] = \"\"\n",
    "\n",
    "            try:\n",
    "                data[\"month\"] = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Month'].lower()\n",
    "            except:\n",
    "                data[\"month\"] = \"\"\n",
    "\n",
    "            keywords = []\n",
    "\n",
    "            if 'KeywordList' in paper['MedlineCitation']:\n",
    "                for keyword_list in paper['MedlineCitation']['KeywordList']:\n",
    "                    keywords.append(keyword_list)\n",
    "\n",
    "            data[\"keywords\"] = keywords\n",
    "\n",
    "            references = []\n",
    "\n",
    "            if 'PubmedData' in paper and 'ReferenceList' in paper['PubmedData']:\n",
    "                for reference in paper['PubmedData']['ReferenceList']:\n",
    "                    for citation in reference['Reference']:\n",
    "                        references.append(citation['Citation'])\n",
    "\n",
    "            data[\"references\"] = references\n",
    "\n",
    "            affiliations = []\n",
    "\n",
    "            if 'AuthorList' in paper['MedlineCitation']['Article']:\n",
    "                author_list = paper['MedlineCitation']['Article']['AuthorList']\n",
    "\n",
    "                for author_info in author_list:\n",
    "                    if 'AffiliationInfo' in author_info:\n",
    "                        for affiliation_info in author_info['AffiliationInfo']:\n",
    "                            affiliation = affiliation_info.get('Affiliation', '')\n",
    "                            affiliations.append(affiliation)\n",
    "                    else:\n",
    "                        ...\n",
    "                        # print(f\"No affiliation information for paper {i}\")\n",
    "\n",
    "            data[\"affiliations\"] = affiliations\n",
    "\n",
    "            identifiers = {}\n",
    "\n",
    "            if 'PubmedData' in paper and 'ArticleIdList' in paper['PubmedData']:\n",
    "                for identifier in paper['PubmedData']['ArticleIdList']:\n",
    "                    data[\"identifiers\"][identifier.attributes['IdType']] = str(identifier)\n",
    "\n",
    "            data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a JSON file\n",
    "with open('papersNew.json', 'w') as json_file:\n",
    "    json.dump(data_list, json_file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataCon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
